{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character-Level LSTM in PyTorch -- Generate names and classify real/fake names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from functools import reduce\n",
    "import random\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T1 - Load in Data\n",
    "\n",
    "Here I created a small dataset (#=6000 names) based on Top 6000 popularity ranks. The txt file is shared to this link also: https://drive.google.com/file/d/1b-0QQGd9nxcI3MvtJYzpBKn67ZxsAG98/view?usp=sharing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open text file and read in data as `text`\n",
    "with open('yob2018-small.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T1 - Tokenization\n",
    "\n",
    "In the second cell, below, I'm creating a couple **dictionaries** to convert the characters to and from integers. Encoding the characters as integers makes it easier to use as input in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the text and map each character to an integer and vice versa\n",
    "\n",
    "# we create two dictonaries:\n",
    "# 1. int2char, which maps integers to characters\n",
    "# 2. char2int, which maps characters to unique integers\n",
    "\n",
    "chars = tuple(set(text))\n",
    "\n",
    "int2char = dict(enumerate(chars))\n",
    "\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T1 - Pre-processing the data\n",
    "\n",
    "The LSTM expects an input that is **one-hot encoded** meaning that each character is converted into an integer (via our created dictionary) and *then* converted into a column vector where only it's corresponsing integer index will have the value of 1 and the rest of the vector will be filled with 0's. Below is the one-hot encoding the data function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # Initialize the the encoded array\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    \n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T1 - Making training batches\n",
    "\n",
    "\n",
    "To train on this data, below is to create mini-batches for training. Remember that we want our batches to be multiple sequences of some desired number of sequence steps. Considering a simple example, our batches would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, n_seqs, n_steps):\n",
    "    '''Create a generator that returns batches of size\n",
    "       n_seqs x n_steps from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       n_seqs: Batch size, the number of sequences per batch\n",
    "       n_steps: Number of sequence steps per batch\n",
    "    '''\n",
    "    \n",
    "    batch_size = n_seqs * n_steps\n",
    "    n_batches = len(arr)//batch_size\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * batch_size]\n",
    "    \n",
    "    # Reshape into n_seqs rows\n",
    "    arr = arr.reshape((n_seqs, -1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        \n",
    "        # The features\n",
    "        x = arr[:, n:n+n_steps]\n",
    "        \n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        \n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+n_steps]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y\n",
    "    #return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T1 - Test the Implementation\n",
    "\n",
    "Now I'll make some data sets and we can check out what's going on as we batch data. Here, as an example, I'm going to use a batch size of 10 and 50 sequence steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = get_batches(encoded, 10, 50)\n",
    "#print(len(encoded))\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[25 52 52 47 31 44 37  9 23  9]\n",
      " [32 31 35  9 52 26 32 41 31 25]\n",
      " [37 24  4  4 31 35 24  4 39 37]\n",
      " [47 24 37 32 32 31 34 47 52 47]\n",
      " [22 47  4  4 31 35 24 41 47 28]\n",
      " [29  4  4  9 32 31 48 24 32  2]\n",
      " [32 24 31 43 37 47 29 41 47 31]\n",
      " [32 24 47 28 31 33 47 19 19 24]\n",
      " [31 20 47 41  9  4 47 31 25 52]\n",
      " [31 35 32 47  4 47 31 35 32 41]]\n",
      "\n",
      "y\n",
      " [[52 52 47 31 44 37  9 23  9 47]\n",
      " [31 35  9 52 26 32 41 31 25 52]\n",
      " [24  4  4 31 35 24  4 39 37 32]\n",
      " [24 37 32 32 31 34 47 52 47 41]\n",
      " [47  4  4 31 35 24 41 47 28 31]\n",
      " [ 4  4  9 32 31 48 24 32  2 47]\n",
      " [24 31 43 37 47 29 41 47 31 43]\n",
      " [24 47 28 31 33 47 19 19 24 31]\n",
      " [20 47 41  9  4 47 31 25 52 32]\n",
      " [35 32 47  4 47 31 35 32 41  9]]\n"
     ]
    }
   ],
   "source": [
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T1 - Defining the LSTM model_1 with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Structure\n",
    "\n",
    "In `__init__` the suggested structure is as follows:\n",
    "* Create and store the necessary dictionaries (this has been done for you)\n",
    "* Define an LSTM layer that takes as params: an input size (the number of characters), a hidden layer size `n_hidden`, a number of layers `n_layers`, a dropout probability `drop_prob`, and a batch_first boolean (True, since we are batching)\n",
    "* Define a dropout layer with `dropout_prob`\n",
    "* Define a fully-connected layer with params: input size `n_hidden` and output size (the number of characters)\n",
    "* Finally, initialize the weights (again, this has been given)\n",
    "\n",
    "Note that some parameters have been named and given in the `__init__` function, and we use them and store them by doing something like `self.drop_prob = drop_prob`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gene_LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, n_steps=100, n_hidden=128, n_layers=1,\n",
    "                       drop_prob=0.5, lr=0.0001):   #  \n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        # Creating character dictionaries\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        ## Define the LSTM\n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
    "             dropout=drop_prob, batch_first=True)   # \n",
    "        \n",
    "        ## Define a dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        ## Define the final, fully-connected output layer\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "        \n",
    "        # Initialize the weights\n",
    "        self.init_weights()\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hc):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hc`. '''\n",
    "        \n",
    "        ## Get x, and the new hidden state (h, c) from the lstm\n",
    "        x, (h, c) = self.lstm(x, hc)\n",
    "        \n",
    "        ## Ppass x through the dropout layer\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Stack up LSTM outputs using view\n",
    "        #x = x.view(x.size()[0]*x.size()[1], self.n_hidden)\n",
    "        x = x.reshape(x.size()[0]*x.size()[1], self.n_hidden)\n",
    "        \n",
    "        ## Put x through the fully-connected layer\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        # Return x and the hidden state (h, c)\n",
    "        return x, (h, c)\n",
    "    \n",
    "    \n",
    "    def predict(self, char, h=None, cuda=False, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "        \n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        if cuda:\n",
    "            self.cuda()\n",
    "        else:\n",
    "            self.cpu()\n",
    "        \n",
    "        if h is None:\n",
    "            h = self.init_hidden(1)\n",
    "        \n",
    "        x = np.array([[self.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(self.chars))\n",
    "        \n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        if cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        h = tuple([each.data for each in h])\n",
    "        out, h = self.forward(inputs, h)\n",
    "\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        \n",
    "        if cuda:\n",
    "            p = p.cpu()\n",
    "        \n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(self.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        p = p.numpy().squeeze()\n",
    "        \n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "            \n",
    "        return self.int2char[char], h\n",
    "    \n",
    "    def init_weights(self):\n",
    "        ''' Initialize weights for fully connected layer '''\n",
    "        initrange = 0.1\n",
    "        \n",
    "        # Set bias tensor to all zeros\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        # FC weights as random uniform\n",
    "        self.fc.weight.data.uniform_(-1, 1)\n",
    "        \n",
    "    def init_hidden(self, n_seqs):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x n_seqs x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        return (weight.new(self.n_layers, n_seqs, self.n_hidden).zero_(),\n",
    "                weight.new(self.n_layers, n_seqs, self.n_hidden).zero_())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_val_list = []\n",
    "loss_list = []\n",
    "def train(net, data, epochs=10, n_seqs=10, n_steps=50, lr=0.001, clip=5, val_frac=0.1, cuda=False, print_every=10):\n",
    "    ''' Training a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: Gene_LSTM network\n",
    "        data: name data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        n_seqs: Number of mini-sequences per mini-batch, aka batch size\n",
    "        n_steps: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        cuda: Train with CUDA on a GPU\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    net.train()\n",
    "    \n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        \n",
    "        h = net.init_hidden(n_seqs)\n",
    "        \n",
    "        for x, y in get_batches(data, n_seqs, n_steps):\n",
    "            \n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            if cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            net.zero_grad()\n",
    "            \n",
    "            output, h = net.forward(inputs, h)\n",
    "     \n",
    "            loss = criterion(output, targets.view(n_seqs*n_steps).type(torch.LongTensor))\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "\n",
    "            opt.step()\n",
    "            \n",
    "            if counter % print_every == 0:\n",
    "                \n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(n_seqs)\n",
    "                val_losses = []\n",
    "         \n",
    "                for x, y in get_batches(val_data, n_seqs, n_steps):\n",
    "            \n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "                    if cuda:\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net.forward(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(n_seqs*n_steps).type(torch.LongTensor))\n",
    "                    #print(val_loss)\n",
    "                    val_losses.append(val_loss.item())\n",
    "                    \n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n",
    "                \n",
    "                loss_val_list.append(np.mean(val_losses))\n",
    "                loss_list.append(loss.item())\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T1 - Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gene_LSTM(\n",
      "  (lstm): LSTM(53, 128, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=128, out_features=53, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "if 'net' in locals():\n",
    "    del net\n",
    "    \n",
    "# Initialize and print the network\n",
    "net1 = Gene_LSTM(chars, n_hidden=128, n_layers=1)\n",
    "\n",
    "print(net1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100... Step: 10... Loss: 4.0479... Val Loss: 4.0414\n",
      "Epoch: 2/100... Step: 20... Loss: 3.9993... Val Loss: 3.9708\n",
      "Epoch: 3/100... Step: 30... Loss: 3.9406... Val Loss: 3.9157\n",
      "Epoch: 4/100... Step: 40... Loss: 3.8751... Val Loss: 3.8633\n",
      "Epoch: 5/100... Step: 50... Loss: 3.8363... Val Loss: 3.8155\n",
      "Epoch: 5/100... Step: 60... Loss: 3.7598... Val Loss: 3.7566\n",
      "Epoch: 6/100... Step: 70... Loss: 3.6897... Val Loss: 3.6894\n",
      "Epoch: 7/100... Step: 80... Loss: 3.6286... Val Loss: 3.6105\n",
      "Epoch: 8/100... Step: 90... Loss: 3.5344... Val Loss: 3.5320\n",
      "Epoch: 9/100... Step: 100... Loss: 3.4494... Val Loss: 3.4408\n",
      "Epoch: 10/100... Step: 110... Loss: 3.4054... Val Loss: 3.3814\n",
      "Epoch: 10/100... Step: 120... Loss: 3.3463... Val Loss: 3.3436\n",
      "Epoch: 11/100... Step: 130... Loss: 3.3308... Val Loss: 3.3254\n",
      "Epoch: 12/100... Step: 140... Loss: 3.2880... Val Loss: 3.2979\n",
      "Epoch: 13/100... Step: 150... Loss: 3.2652... Val Loss: 3.2729\n",
      "Epoch: 14/100... Step: 160... Loss: 3.2581... Val Loss: 3.2695\n",
      "Epoch: 15/100... Step: 170... Loss: 3.2658... Val Loss: 3.2571\n",
      "Epoch: 15/100... Step: 180... Loss: 3.2345... Val Loss: 3.2407\n",
      "Epoch: 16/100... Step: 190... Loss: 3.2372... Val Loss: 3.2296\n",
      "Epoch: 17/100... Step: 200... Loss: 3.2068... Val Loss: 3.2125\n",
      "Epoch: 18/100... Step: 210... Loss: 3.1910... Val Loss: 3.2021\n",
      "Epoch: 19/100... Step: 220... Loss: 3.2020... Val Loss: 3.1946\n",
      "Epoch: 20/100... Step: 230... Loss: 3.1791... Val Loss: 3.1884\n",
      "Epoch: 20/100... Step: 240... Loss: 3.1695... Val Loss: 3.1715\n",
      "Epoch: 21/100... Step: 250... Loss: 3.1782... Val Loss: 3.1568\n",
      "Epoch: 22/100... Step: 260... Loss: 3.1607... Val Loss: 3.1795\n",
      "Epoch: 23/100... Step: 270... Loss: 3.1371... Val Loss: 3.1592\n",
      "Epoch: 24/100... Step: 280... Loss: 3.1321... Val Loss: 3.1361\n",
      "Epoch: 25/100... Step: 290... Loss: 3.1335... Val Loss: 3.1504\n",
      "Epoch: 25/100... Step: 300... Loss: 3.1077... Val Loss: 3.1358\n",
      "Epoch: 26/100... Step: 310... Loss: 3.1317... Val Loss: 3.1185\n",
      "Epoch: 27/100... Step: 320... Loss: 3.1250... Val Loss: 3.1127\n",
      "Epoch: 28/100... Step: 330... Loss: 3.0970... Val Loss: 3.1047\n",
      "Epoch: 29/100... Step: 340... Loss: 3.0785... Val Loss: 3.1095\n",
      "Epoch: 30/100... Step: 350... Loss: 3.0931... Val Loss: 3.1013\n",
      "Epoch: 30/100... Step: 360... Loss: 3.0691... Val Loss: 3.0991\n",
      "Epoch: 31/100... Step: 370... Loss: 3.0820... Val Loss: 3.0580\n",
      "Epoch: 32/100... Step: 380... Loss: 3.0584... Val Loss: 3.0634\n",
      "Epoch: 33/100... Step: 390... Loss: 3.0329... Val Loss: 3.0509\n",
      "Epoch: 34/100... Step: 400... Loss: 3.0313... Val Loss: 3.0457\n",
      "Epoch: 35/100... Step: 410... Loss: 3.0322... Val Loss: 3.0372\n",
      "Epoch: 35/100... Step: 420... Loss: 3.0094... Val Loss: 3.0412\n",
      "Epoch: 36/100... Step: 430... Loss: 3.0306... Val Loss: 3.0343\n",
      "Epoch: 37/100... Step: 440... Loss: 2.9847... Val Loss: 3.0432\n",
      "Epoch: 38/100... Step: 450... Loss: 2.9937... Val Loss: 3.0195\n",
      "Epoch: 39/100... Step: 460... Loss: 2.9785... Val Loss: 3.0020\n",
      "Epoch: 40/100... Step: 470... Loss: 2.9864... Val Loss: 2.9972\n",
      "Epoch: 40/100... Step: 480... Loss: 2.9590... Val Loss: 2.9787\n",
      "Epoch: 41/100... Step: 490... Loss: 2.9591... Val Loss: 2.9680\n",
      "Epoch: 42/100... Step: 500... Loss: 2.9421... Val Loss: 2.9824\n",
      "Epoch: 43/100... Step: 510... Loss: 2.9196... Val Loss: 2.9530\n",
      "Epoch: 44/100... Step: 520... Loss: 2.9015... Val Loss: 2.9321\n",
      "Epoch: 45/100... Step: 530... Loss: 2.9311... Val Loss: 2.9351\n",
      "Epoch: 45/100... Step: 540... Loss: 2.8894... Val Loss: 2.9185\n",
      "Epoch: 46/100... Step: 550... Loss: 2.9220... Val Loss: 2.8882\n",
      "Epoch: 47/100... Step: 560... Loss: 2.8681... Val Loss: 2.8888\n",
      "Epoch: 48/100... Step: 570... Loss: 2.8664... Val Loss: 2.8910\n",
      "Epoch: 49/100... Step: 580... Loss: 2.8440... Val Loss: 2.8700\n",
      "Epoch: 50/100... Step: 590... Loss: 2.8616... Val Loss: 2.8714\n",
      "Epoch: 50/100... Step: 600... Loss: 2.8253... Val Loss: 2.8784\n",
      "Epoch: 51/100... Step: 610... Loss: 2.8311... Val Loss: 2.8399\n",
      "Epoch: 52/100... Step: 620... Loss: 2.8438... Val Loss: 2.8250\n",
      "Epoch: 53/100... Step: 630... Loss: 2.7803... Val Loss: 2.8300\n",
      "Epoch: 54/100... Step: 640... Loss: 2.7777... Val Loss: 2.8083\n",
      "Epoch: 55/100... Step: 650... Loss: 2.7981... Val Loss: 2.7993\n",
      "Epoch: 55/100... Step: 660... Loss: 2.7646... Val Loss: 2.8039\n",
      "Epoch: 56/100... Step: 670... Loss: 2.7701... Val Loss: 2.7986\n",
      "Epoch: 57/100... Step: 680... Loss: 2.7750... Val Loss: 2.7888\n",
      "Epoch: 58/100... Step: 690... Loss: 2.7131... Val Loss: 2.7714\n",
      "Epoch: 59/100... Step: 700... Loss: 2.7489... Val Loss: 2.7756\n",
      "Epoch: 60/100... Step: 710... Loss: 2.7541... Val Loss: 2.7565\n",
      "Epoch: 60/100... Step: 720... Loss: 2.7203... Val Loss: 2.7536\n",
      "Epoch: 61/100... Step: 730... Loss: 2.7270... Val Loss: 2.7458\n",
      "Epoch: 62/100... Step: 740... Loss: 2.7262... Val Loss: 2.7523\n",
      "Epoch: 63/100... Step: 750... Loss: 2.6894... Val Loss: 2.7383\n",
      "Epoch: 64/100... Step: 760... Loss: 2.7073... Val Loss: 2.7451\n",
      "Epoch: 65/100... Step: 770... Loss: 2.7099... Val Loss: 2.7337\n",
      "Epoch: 65/100... Step: 780... Loss: 2.6687... Val Loss: 2.7247\n",
      "Epoch: 66/100... Step: 790... Loss: 2.6836... Val Loss: 2.7131\n",
      "Epoch: 67/100... Step: 800... Loss: 2.6739... Val Loss: 2.7025\n",
      "Epoch: 68/100... Step: 810... Loss: 2.6442... Val Loss: 2.7013\n",
      "Epoch: 69/100... Step: 820... Loss: 2.6613... Val Loss: 2.7082\n",
      "Epoch: 70/100... Step: 830... Loss: 2.6641... Val Loss: 2.6843\n",
      "Epoch: 70/100... Step: 840... Loss: 2.6300... Val Loss: 2.6848\n",
      "Epoch: 71/100... Step: 850... Loss: 2.6359... Val Loss: 2.6909\n",
      "Epoch: 72/100... Step: 860... Loss: 2.6391... Val Loss: 2.6710\n",
      "Epoch: 73/100... Step: 870... Loss: 2.6077... Val Loss: 2.6703\n",
      "Epoch: 74/100... Step: 880... Loss: 2.6190... Val Loss: 2.6621\n",
      "Epoch: 75/100... Step: 890... Loss: 2.6281... Val Loss: 2.6814\n",
      "Epoch: 75/100... Step: 900... Loss: 2.5928... Val Loss: 2.6552\n",
      "Epoch: 76/100... Step: 910... Loss: 2.6032... Val Loss: 2.6714\n",
      "Epoch: 77/100... Step: 920... Loss: 2.6088... Val Loss: 2.6480\n",
      "Epoch: 78/100... Step: 930... Loss: 2.5717... Val Loss: 2.6346\n",
      "Epoch: 79/100... Step: 940... Loss: 2.5819... Val Loss: 2.6291\n",
      "Epoch: 80/100... Step: 950... Loss: 2.6011... Val Loss: 2.6447\n",
      "Epoch: 80/100... Step: 960... Loss: 2.5748... Val Loss: 2.6371\n",
      "Epoch: 81/100... Step: 970... Loss: 2.5640... Val Loss: 2.6234\n",
      "Epoch: 82/100... Step: 980... Loss: 2.5748... Val Loss: 2.6090\n",
      "Epoch: 83/100... Step: 990... Loss: 2.5330... Val Loss: 2.6283\n",
      "Epoch: 84/100... Step: 1000... Loss: 2.5650... Val Loss: 2.5817\n",
      "Epoch: 85/100... Step: 1010... Loss: 2.5709... Val Loss: 2.6019\n",
      "Epoch: 85/100... Step: 1020... Loss: 2.5425... Val Loss: 2.5781\n",
      "Epoch: 86/100... Step: 1030... Loss: 2.5372... Val Loss: 2.5958\n",
      "Epoch: 87/100... Step: 1040... Loss: 2.5593... Val Loss: 2.5982\n",
      "Epoch: 88/100... Step: 1050... Loss: 2.5109... Val Loss: 2.5817\n",
      "Epoch: 89/100... Step: 1060... Loss: 2.5235... Val Loss: 2.5714\n",
      "Epoch: 90/100... Step: 1070... Loss: 2.5672... Val Loss: 2.5690\n",
      "Epoch: 90/100... Step: 1080... Loss: 2.5048... Val Loss: 2.5846\n",
      "Epoch: 91/100... Step: 1090... Loss: 2.5012... Val Loss: 2.5631\n",
      "Epoch: 92/100... Step: 1100... Loss: 2.5210... Val Loss: 2.5575\n",
      "Epoch: 93/100... Step: 1110... Loss: 2.4740... Val Loss: 2.5527\n",
      "Epoch: 94/100... Step: 1120... Loss: 2.4939... Val Loss: 2.5161\n",
      "Epoch: 95/100... Step: 1130... Loss: 2.5181... Val Loss: 2.5576\n",
      "Epoch: 95/100... Step: 1140... Loss: 2.4765... Val Loss: 2.5532\n",
      "Epoch: 96/100... Step: 1150... Loss: 2.4757... Val Loss: 2.5471\n",
      "Epoch: 97/100... Step: 1160... Loss: 2.5115... Val Loss: 2.5371\n",
      "Epoch: 98/100... Step: 1170... Loss: 2.4640... Val Loss: 2.5368\n",
      "Epoch: 99/100... Step: 1180... Loss: 2.4845... Val Loss: 2.5162\n",
      "Epoch: 100/100... Step: 1190... Loss: 2.4808... Val Loss: 2.5240\n",
      "Epoch: 100/100... Step: 1200... Loss: 2.4596... Val Loss: 2.5250\n"
     ]
    }
   ],
   "source": [
    "n_seqs, n_steps = 10, 300\n",
    "\n",
    "train(net1, encoded, epochs=100, n_seqs=n_seqs, n_steps=n_steps, lr=0.0001, cuda=False, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x11f041da0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nOzdd3hUVfrA8e87k0YakEINIfQWIIRIV6o0BRsKKKAoYFu7/mTVVZfVXdeKbVUsiIogolgQRVCQXhJ67yUQIAkQSEif8/vjDi0kIUAmk/J+nifPzsw999z3BjfvnHPuOUeMMSillKq4bO4OQCmllHtpIlBKqQpOE4FSSlVwmgiUUqqC00SglFIVnCYCpZSq4DQRqBIjInYRSRWR8FIQyyIRucvVdYvInSLyqyviEJH6IpJ6eVEqdZYmAlUg5x/t0z8OEUk/5/0dl1qfMSbXGONvjNnniniLg4gMF5Gd+XzuJSJJItL3UuozxkwyxvQrptjiRaTbOXXvMsb4F0fdea7jISJGRCKKu25VOmkiUAVy/tH2d/6x2QcMOOezyXnLi4hHyUdZ7L4DQkWkS57P+wNZwJySD0kp19JEoC6biLwkIt+IyBQROQkME5GOIrJMRI6LSIKIvCMins7y533TFJGvnMd/FZGTIrJUROoVcC2biEwXkUPOuueLSLNzjhdal4j0FZGtIpIiIm8Dkt91jDGngOnAiDyHRgBfGWNyRSRYRGaJSKKIHBORn0WkdgFxjxKR+UWJQ0Qaicg8EUl2tj6+FJHKzmNTgFrAr84W2eMi0lBEzDnnh4nITBE5KiLbReTuPP9WU5y/p5MiskFEovOLuTDOf4fnRWSviBwRkc9FJNB5zFdEvnbGf1xEVohIiPPYPSKyx3ntXSIy5FKvrVxHE4G6UjcBXwOVgW+AHOARIAToDPQF7i3k/NuBfwBBWK2OfxVSdibQCKgBbAC+LEpdIlIN64/7WGdc8UD7Qq4zCbhNRHyc51cFrgO+cB63AR8D4UBdIBt4u5D6KGIcArwE1ASaA/Wd94MxZihwEOjnbJG9mc8lvgF2YyWMwcCrItL1nOM3Yv3OqgC/Au9cLOZ8jAKGAd2ABkBVzt77SMAXCAOCgQeADGeieBO41hgTgPXfxbrLuLZyEU0E6kotMsb8bIxxGGPSjTErjTHLjTE5xphdwASgayHnTzfGxBpjsoHJQFR+hZz1f26MOWmMyQBeBNqKiF8R6roeWGOMmeE89gaQWEhMC4CjwEDn+yHABmPMBmcsic660o0xJ4B/X+QeTys0DmPMNmPMH8aYLGPMEeCtItaLs/XTDhhrjMkwxqwCJgLDzyn2lzFmtjEmFysh5Pu7vog7gNeNMbuNMSeBZ4DbRcSGlRBDgIbO8aBYY8zpwWwDRIqIjzEmwRiz6TKurVxEE4G6UvvPfSMiTUXkF2cXzglgHNYfh4IcOuf1KSDfwU+xnjh61dmtcALY4Tx0bt0F1VXr3DiNMQ6sb+P5MtZKjF9ytntoOFYr4XQsfiLyiYjsc8byJ4Xf42mFxiEiNURkmogccNb7eRHrPV13kjEm7ZzP9gLndlnl/f2cm0SLqpaz3nOv4QWEYsU7Fzh9D6+IiIczWQ4FHgQOObuvGl/GtZWLaCJQVyrv8rUfYXXbNDTGBALPU0B//CUagTVg2wOrG6qh8/Oi1J0A1Dn9xvntNewi53wB9BaRTkAMMOWcY/8H1APaOe+xR1FuoAhx/BfIBFo6672L8++vsKWCDwIheVpI4cCBIsZWVAexusPOvUYWkOhsybxojGkGdMHqNrwDwBjzqzGmF1a31w6s/05UKaGJQBW3ACAFSHMO5hY2PnCp9WYCyVj90C9fwrkzgSgRucH5ZNNjWN9gC2SM2Qksxxr/+NUYc25XUgDWN+pjIhKMleyKI44AIA1IEZE6wJN5zj+MNW6QX7y7gVjg3yLiLSJRWH32FzzddQm8RcTnnB87VkJ8XEQiRCQA699hijHGISI9RCTSmeBOYHUV5YpITREZICK+WEkjDci9grhUMdNEoIrbE8CdwEmsb33fFFO9E7G+jR4ENgJLinqiMeYw1uDpa1iJJBzrj/zFTML69vtFns/fxGqVJDvjKHDC2CXG8QJWP38K8BPWo6zn+jfwT+cTOY/mc4nBWIPph7AGpZ8xxswrSmwF2AKkn/MzHGuQ/BtgIbAL69/5EWf5WsD3WElgI1Y30RTADjyF1SJKBjoBf7uCuFQxE92YRimlKjZtESilVAWniUAppSo4TQRKKVXBaSJQSqkKrswtEhYSEmIiIiLcHYZSSpUpcXFxScaYfB+bLnOJICIigtjYWHeHoZRSZYqI7C3omHYNKaVUBaeJQCmlKjhNBEopVcGVuTECpVT5kZ2dTXx8PBkZGe4Opdzw8fEhLCwMT0/PIp+jiUAp5Tbx8fEEBAQQERGBSHEsUluxGWNITk4mPj6eevXy3ewvX9o1pJRym4yMDIKDgzUJFBMRITg4+JJbWJoIlFJupUmgeF3O77PCJIKdian88+eNZOc63B2KUkqVKhUmEaRsnkfvFaOYu3q7u0NRSpUSycnJREVFERUVRY0aNahdu/aZ91lZWUWqY+TIkWzdutXFkbpWhRksjgoPxmbfxBuLfqBfzNPuDkcpVQoEBwezZs0aAF588UX8/f158snzN4YzxmCMwWbL/3vzxIkTXR6nq1WYFoGtTjsyPAIJT1rExoMp7g5HKVWK7dixg8jISO677z6io6NJSEhgzJgxxMTE0KJFC8aNG3embJcuXVizZg05OTlUqVKFsWPH0rp1azp27MiRI0fceBdF5/IWgXOf01jggDHm+jzHvLG2AWyLtYXdYGPMHpcEYvfA1rAn3Tb/yRuLd/PKrVEuuYxS6vL88+eNbDp4oljrbF4rkBcGtLisczdt2sTEiRP58MMPAXjllVcICgoiJyeH7t27M2jQIJo3b37eOSkpKXTt2pVXXnmFxx9/nM8++4yxY8de8X24Wkm0CB4BNhdw7B7gmDGmIfAW8F9XBuLVrC+hksL2tYs4fqpo/X9KqYqpQYMGXHXVVWfeT5kyhejoaKKjo9m8eTObNm264JxKlSrRr18/ANq2bcuePXtKKtwr4tIWgYiEAdcBLwOP51PkBuBF5+vpwHsiIsZVGyk37IVB6GxWMy12P2OuaeCSyyilLt3lfnN3FT8/vzOvt2/fzttvv82KFSuoUqUKw4YNy/dZfS8vrzOv7XY7OTk5JRLrlXJ1i2A88H9AQc9s1gb2AxhjcoAUIDhvIREZIyKxIhKbmJh4+dH4hSC123K97wamrtyPq/KNUqp8OXHiBAEBAQQGBpKQkMDs2bPdHVKxclkiEJHrgSPGmLjCiuXz2QV/nY0xE4wxMcaYmNDQfPdVKLpGvWmUvZXjiQlsLOb+SKVU+RQdHU3z5s2JjIxk9OjRdO7c2d0hFStx1bdiEfkPMBzIAXyAQOB7Y8ywc8rMBl40xiwVEQ/gEBBaWNdQTEyMuaKNaQ6uhgndeDLnAYI6jeCZ/s0uvy6l1BXZvHkzzZrp/weLW36/VxGJM8bE5FfeZS0CY8zfjTFhxpgIYAjw57lJwOkn4E7n60HOMq7tr6nRGvyqMThwAz+vPYjDod1DSqmKrcTnEYjIOBEZ6Hz7KRAsIjuwBpNd/5yVzQbNBhCdsZzUlKOs3HPU5ZdUSqnSrEQSgTFm/uk5BMaY540xPzlfZxhjbjXGNDTGtDPG7CqJeIi6A7sjkxu9VvDj2oMlckmllCqtKszM4vPUjoaQxtzlt4RZ6xPIytGF6JRSFVfFTAQiEHU7DdI3UCV9H4t3JLk7IqWUcpuKmQgAWg3GiI0hXov5fdMhd0ejlFJuU3ETQWAtpH53bvNczNyNCeTq00NKVTjdunW7YHLY+PHjeeCBBwo8x9/fH4CDBw8yaNCgAuu92GPu48eP59SpU2fe9+/fn+PHjxc19GJVcRMBQNTtBOUcpnH6albvO+buaJRSJWzo0KFMnTr1vM+mTp3K0KFDL3purVq1mD59+mVfO28imDVrFlWqVLns+q5ExU4ETa/H+FRhqMd8ft902N3RKKVK2KBBg5g5cyaZmZkA7Nmzh4MHDxIVFUXPnj2Jjo6mZcuW/Pjjjxecu2fPHiIjIwFIT09nyJAhtGrVisGDB5Oenn6m3P33339m+eoXXngBgHfeeYeDBw/SvXt3unfvDkBERARJSdZ45ZtvvklkZCSRkZGMHz/+zPWaNWvG6NGjadGiBb179z7vOleiwmxMky9PH6TVYPqs+IwJG7Zh+jXV/VOVcpdfx8Kh9cVbZ42W0O+VAg8HBwfTrl07fvvtN2644QamTp3K4MGDqVSpEjNmzCAwMJCkpCQ6dOjAwIEDC/z78MEHH+Dr68u6detYt24d0dHRZ469/PLLBAUFkZubS8+ePVm3bh0PP/wwb775JvPmzSMkJOS8uuLi4pg4cSLLly/HGEP79u3p2rUrVatWZfv27UyZMoWPP/6Y2267je+++45hw/LO0710FbtFABA9Ak+yiUmZw/Yjqe6ORilVws7tHjrdLWSM4ZlnnqFVq1b06tWLAwcOcPhwwb0GCxYsOPMHuVWrVrRq1erMsWnTphEdHU2bNm3YuHFjvstXn2vRokXcdNNN+Pn54e/vz80338zChQsBqFevHlFR1l4qxbnMdcVuEQDUiCS7RhuGHPyT2esTaFw9wN0RKVUxFfLN3ZVuvPFGHn/8cVatWkV6ejrR0dF8/vnnJCYmEhcXh6enJxEREfkuO32u/FoLu3fv5vXXX2flypVUrVqVu+6666L1FLbKjre395nXdru92LqGtEUAeF41ksa2A+xYPU/XHlKqgvH396dbt27cfffdZwaJU1JSqFatGp6ensybN4+9e/cWWsc111zD5MmTAdiwYQPr1q0DrOWr/fz8qFy5MocPH+bXX389c05AQAAnT57Mt64ffviBU6dOkZaWxowZM7j66quL63bzpYkAIPIWsu2+dE75hVkbEtwdjVKqhA0dOpS1a9cyZMgQAO644w5iY2OJiYlh8uTJNG3atNDz77//flJTU2nVqhWvvvoq7dq1A6B169a0adOGFi1acPfdd5+3fPWYMWPo16/fmcHi06Kjo7nrrrto164d7du3Z9SoUbRp06aY7/h8LluG2lWueBnqAjh+episVVO4ze8zvnv8OjztmiOVcjVdhto1Ss0y1GWNrd0YfMiiw/FfmB4X7+5wlFKqxGgiOK1GJCaiC6O95/LunM1kZOe6OyKllCoRmgjOIe3vI9SRSMu0pcxYfcDd4ShVIZS17unS7nJ+n5oIztW4H6ZyHcZ4z+GvrYnujkapcs/Hx4fk5GRNBsXEGENycjI+Pj6XdJ7L5hGIiA+wAPB2Xme6MeaFPGXCgUlAFcAOjDXGzHJVTBdl90DajabtnOf59844ch3R2G0601gpVwkLCyM+Pp7ERP3iVVx8fHwICwu7pHNcOaEsE+hhjEkVEU9gkYj8aoxZdk6Z54BpxpgPRKQ5MAuIcGFMF9dmOLl//Iv+WX+y/sBtRNVxzyJQSlUEnp6e1KtXz91hVHiu3LzeGGNOr9ng6fzJ2/4zQKDzdWXA/ftG+gaRU78X19uXsnib7lOglCr/XDpGICJ2EVkDHAHmGGOW5ynyIjBMROKxWgMPFVDPGBGJFZHYkmhCekfdRnU5TvKm+S6/llJKuZtLE4ExJtcYEwWEAe1EJDJPkaHA58aYMKA/8KWIXBCTMWaCMSbGGBMTGhrqypAtjfuSafOlSeJvnMrKcf31lFLKjUrkqSFjzHFgPtA3z6F7gGnOMksBHyAEd/Py5Xj4tfSRFazYod1DSqnyzWWJQERCRaSK83UloBewJU+xfUBPZ5lmWImgVDw+ULX97VSRNA6t+sXdoSillEu5skVQE5gnIuuAlVhjBDNFZJyIDHSWeQIYLSJrgSnAXaaUPFDs1bgnJ22BVN87092hKKWUS7ns8VFjzDrggiXzjDHPn/N6E9A5b5lSwe7JvhrX0v7ATBKSj1IzOMjdESmllEvozOJCBLa5CV/JZOsS981xU0opV9NEUIiwqGtJoxKyTROBUqr80kRQCPH0YXeVjjQ7sYSMrGx3h6OUUi6hieAibE37U02OsSn2L3eHopRSLqGJ4CLqd7qJHGMjdd1P7g5FKaVcQhPBRfgEhrC9UktqH56vS+UqpcolTQRFkBbRmwZmL3t3bnJ3KEopVew0ERRBWIdbADiw7Hs3R6KUUsVPE0ER1IhoRrytNl77dMBYKVX+aCIoouM1OtEscz37E1PcHYpSShUrTQRFVL11b/wlg7hlf7g7FKWUKlaaCIootGUvHAipm/90dyhKKVWsNBEUlW8Qyf5NaHAyjv1HT7k7GqWUKjaaCC6Bd+PuRNu2MXvNLneHopRSxUYTwSUIbNYTb8lhz+p57g5FKaWKjSaCSxHekVzxoNaxFRxKyXB3NEopVSxcuVWlj4isEJG1IrJRRP5ZQLnbRGSTs8zXroqnWHj7k16tDZ1sG1iz/5i7o1FKqWLhyhZBJtDDGNMaiAL6ikiHcwuISCPg70BnY0wL4FEXxlMsfBp3p6XsZtOu/e4ORSmlioXLEoGxpDrfejp/8q7aNhp43xhzzHnOEVfFU1w8GnTDLobcXQvdHYpSShULl44RiIhdRNYAR7A2r1+ep0hjoLGILBaRZSLSt4B6xohIrIjEJiYmujLkiwu7iiybDzWPLic71+HeWJRSqhi4NBEYY3KNMVFAGNBORCLzFPEAGgHdgKHAJyJSJZ96JhhjYowxMaGhoa4M+eI8vDgeEkN7NrD10En3xqKUUsWgRJ4aMsYcB+YDeb/xxwM/GmOyjTG7ga1YiaFU82rUnUa2A2zZvs3doSil1BVz5VNDoae/3YtIJaAXsCVPsR+A7s4yIVhdRaV+tlblFj0ByNym8wmUUmWfhwvrrglMEhE7VsKZZoyZKSLjgFhjzE/AbKC3iGwCcoGnjDHJLoypWEiN1qTaAgg+stTdoSil1BVzWSIwxqwD2uTz+fPnvDbA486fssNm43BQO1omruFYaiZV/b3dHZFSSl02nVl8maR+V2pLMlu3rHV3KEopdUU0EVymGlF9AEjdpPsTKKXKNk0El8m3ZhMSJQT/BB0nUEqVbZoILpcIh6u0JvzUBp1YppQq0zQRXAEJb08tSWbrtrxPxSqlVNmhieAK1IrsBkDChgXuDUQppa6AJoIrULV+NBl4YfbnXUJJKaXKDk0EV8LuyUG/ZtQ4sY5cR96FVZVSqmzQRHCFsmtdRVOzm23xpX4FbaWUypcmgisU0uwavCSX3esWuzsUpZS6LJoIrlBwk84AZO7W+QRKqbJJE8GV8gvhiFcYVY6uwVo6SSmlyhZNBMUgNTSaSMcWNh884e5QlFLqkmkiKAbVm19DqJzgp3m6j7FSquzRRFAM/FpYC9DZt/zMwePpbo5GKaUujSaC4lAlnMyaMVxvW8InC3e7OxqllLokrtyq0kdEVojIWhHZKCL/LKTsIBExIhLjqnhczTvqNprZ9rFy5RKOpWW5OxyllCoyV7YIMoEexpjWQBTQV0Q65C0kIgHAw0DZXqeh+Y0YsdHLsYgvlu51dzRKKVVkLksExpLqfOvp/Mnv+cp/Aa8CGa6KpUQEVEcirmawz3KmrtiLQ5ecUEqVES4dIxARu4isAY4Ac4wxy/McbwPUMcbMvEg9Y0QkVkRiExMTXRjxFYq8hRo5Bwk5uZnYvcfcHY1SShWJSxOBMSbXGBMFhAHtRCTy9DERsQFvAU8UoZ4JxpgYY0xMaGio6wK+Us0GYGye3Oy5lJ/WHnB3NEopVSQl8tSQMeY4MB/oe87HAUAkMF9E9gAdgJ/K8oAxvkFIk34M8ZjPknXbdOcypVSZ4MqnhkJFpIrzdSWgF3BmKy9jTIoxJsQYE2GMiQCWAQONMbGuiqlEdH8WH0c6d2R9y+IdSe6ORimlLsqVLYKawDwRWQesxBojmCki40RkoAuv617VmuKIup0RHr+zaGWcu6NRSqmL8nBVxcaYdUCbfD5/voDy3VwVS0mz93iW3LXfErX9XTKye+PjaXd3SEopVSCdWewKgbU40uJurpfFfPrZB5zIyHZ3REopVSBNBC5S+/q/k+jflHsP/oMP3nyRTboyqVKqlNJE4CLiU5nQh+aSVrszT2e9x5+fjCU9K9fdYSml1AU0EbiSdwCV75lBUsQA/uaYzOJZX7o7IqWUukCREoGINBARb+frbiLy8OlHQ9VF2D0JueMTdnk0oN2aZ8lK1nWIlFKlS1FbBN8BuSLSEPgUqAd87bKoyhtPHxL7foSYXE58OQxydfBYKVV6FDUROIwxOcBNwHhjzGNY8wRUEbVrG8P/Ah4h5Pg6HL894+5wlFLqjKImgmwRGQrcCZxeIM7TNSGVTyJC67538XFOf2wrJ0DcJHeHpJRSQNETwUigI/CyMWa3iNQDvnJdWOVT7+Y1WFb/Yf5ytMLxyxOwd6m7Q1JKKcSYS1s3X0SqYi0dvc41IRUuJibGxMaW3eWITmXlMOrDubyc/CgRtsNI1XpQrRm0HkJu4+s4lp5NiL+3u8NUSpUzIhJnjMl3Uc+iPjU0X0QCRSQIWAtMFJE3izPIisLXy4N37+7O037/4r2cG4nNqEXa3jj4ZhjLX+rBkP98xbr44+4OUylVgRS1a6iyMeYEcDMw0RjTFms1UXUZgv29eefeAaS0f4q/5TxGq2Ov8q+c4bRhKzM9xzL76/Fk5egS1kqpklGkriERWQ/0BiYBzxpjVorIOmNMK1cHmFdZ7xrKy+EwbDiYQo3KPlQzxzj65Z0EJS5ndc3BtBn1Pth1TF4pdeWuuGsIGAfMBnY6k0B9YHtxBViR2WxCq7AqVAvwgcCaBN03i3lVb6VNwjccHd+ZGd9+wcszN3JSF65TSrlIkRKBMeZbY0wrY8z9zve7jDG3uDa0CsruQdSo//GU7SlSU45y08aH6Lp8NO/NmO/uyJRS5VRRB4vDRGSGiBwRkcMi8p2IhLk6uIqqqp8XTz36JFn3Lye793+4ynM3o7bcQ9zi390dmlKqHCpq19BE4CegFlAb+Nn5WYFExEdEVojIWhHZKCL/zKfM4yKySUTWicgfIlL3Um+gvKoW6EPDmsF4dnoARs0l21aJyDm3k75kAuTmuDs8pVQ5UtREEGqMmWiMyXH+fA6EXuScTKCHMaY1EAX0FZEOecqsBmKcg87TgVcvIfYKw7tWC47d8SurHI2o9PtTON67CtZMgZwsTmRkc+REhrtDVEqVYUVNBEkiMkxE7M6fYUByYScYS6rzrafzx+QpM88Yc8r5dhmg3U0FaNGwPlt7T2ZM9uPsSjHww31kvNKQX1+5nSff+IhN8cfcHaJSqowq6uOj4cB7WMtMGGAJ8LAxZt9FzrMDcUBD4H1jzNOFlH0POGSMeSmfY2OAMQDh4eFt9+6tuEs5L96RxCNfx9EiI46b7Qvp6xGHt8nkGIF4NeuLX8+nIaShu8NUSpUyhT0+eslLTJxT6aPGmPFFLFsFmAE8ZIzZkM/xYcDfgK7GmMzC6ipv8wgux6GUDCYs2EWfFtVpX9uLgyt/Jm7u13QnDl9vD2y3ToSG1ny/rYdOUrtqJfwdqbBnETTpDzbdj0ipisZViWCfMSb8Esq/AKQZY17P83kv4F2sJHDkYvVoIsjf8l3JjP3sFz6wv0ZjiWdds8d5IbE7a+NTaB5i53u/V/E5HAfRd8L14zUZKFXBFMeEsnzrvchFQ0/vYiYilbCWpNiSp0wb4CNgYFGSgCpY+/rBfPH4IL5t9SlzHDFEbX6dscde4LmrqzI29b94Hl7F0fC+sGoS/PwwOHQJC6WUxWUtAhFphbUkhR0r4UwzxowTkXFArDHmJxGZC7QEEpyn7TPGDCzsutoiuLikkxmkL/6AsJX/QYwDHNm86X0/H6Zew59tlxC27l2rZTDgbZBC87lSqpy47K4hETlJnid9Th8CKhljPIonxKLTRHAJDm+CX56AJn05FnU/gz5cQlJqFvPaLCBo1bvQdSx0/ztkpsKf/4LsdOj9L/Cp7O7IlVLFzCVjBO6iieDy7T96ipv+txgfDxtzGkyj0sap0OUx2PgDHNsDYoPA2nDzR1C3k7vDVUoVI1eNEagypk6QL5/eeRVJaVlErb6OOM+2sOgtjqZm8GbYeH5t/znY7PD5dbD4bbYdOsG02P0kniz0QS6lVBmnLYIKaHPCCX5Zl8DanfHUPfQ7Cz07keXhT0JKBu/f0ojrdr8Mm37gB+nBU+l3kSMeXBURxIsDWtC8VqC7w1dKXYbCWgQl3sev3K9ZzUCa1QwEmgA9AcjOdXDHx8t54qedBN35Ojt2eDM86xt6VT/A2srd+fBgfcZ8cYpfHrmGypV0jwSlyhNtEagzEk9mMuDdRRw6kYGnXfilRxKNd3wKCWsA2G1qsDZkIDfcMAhJWAvxKyGoPkQPh8q6OohSpZkOFqsiW7P/OPd/FcdTfZpwc7Tzj3vqEdg2m4N/fUqtlNVnC/tVg7RE6xHUiKshpBEE1ITIm60EoZQqNTQRqEtijEHymV/gcBienvA96fHrGTRwAN3atYVjezkwbwL27b9R3SQjGccgqAE8uALs2vOoVGmhTw2pS5JfEgBrW81nRwzgYK1rGTnjEJOW7OH1FRlcvbITHY6N465q08gd9AUc3Ylj/besiz9OWfuioVRFpIlAXZIqvl5MHtWBHk2q8cJPG3lv3g5uiQ7j+eub89e2RF7YFkF6UHMSfhrHTe8t4LXZW90dslLqIrTtri5ZJS87Hw1vywfzd9Kouj99I2sCcPhkBh/9tYtEex8+8nyLsWEbeHm+nbCqvtzevsjrEyqlSpgmAnVZPOw2HurZ6LzPnu7TlIysXMTUIffgHEZlf0PDkA1EzIrjxPJqBN74GoTn3aROKeVuOlisXGPLLJg6FOMdwApHU+pm7aKGJEPUMGh7J9RqA3adj6BUSdEJZarkNe0Pj6xDAmtT52Q2g96Zw4P2GQxZNxVZ8xV4+kH9rtB2pLWJjs0GaUmQfhyCG5xdFTVxG8SvgFZD9CkkpVxE/5+lXKdqXZzwyWcAACAASURBVABqVfHgjWGdueMTD5Y1uJ0XWh0n8NAy7Ft/RrbOwhFYBxsGTsRb5wU1gGYD4NB62PmH9dnBNdD/NV02WykX0K4hVWK+XLqHf/y48cx7T3LoY1vJzfaFhISE0jLmGsSzEtkbf8a+dyGZ3kFU6nSv1VJY8RH0+Td0fNB9N6BUGaZdQ6pUGNahLg2rBbAnOY2jaVlk5jjw9Ypk1pFbmR4Xz6PpjbinbT2Gr2zOjozh+Nr8WdCpNz52gZMJMPtZa5nsFje6+1aUKldclghExAdYAHg7rzPdGPNCnjLewBdAWyAZGGyM2eOqmJR7iQgdGwTTsUHweZ+fbpWOn7udGasPcOBYOvdcE8lHC3YxY/UBhrYLh5snwKSBMH0knDwEHe5zxy0oVS65ckJZJtDDGNMaiAL6ikjeZwfvAY4ZYxoCbwH/dWE8qpQSEf5zc0t6NK3G/qOneHtIG8b2a0pk7UA+WbgLh8OAZyUY8QM07ge/PW3tvHZsL5Sxrk2lSiOXJQJjSXW+9XT+5P1/7Q1Y+xoDTAd6SkHrG6hyzdNuY8Lwtix8ugfXtaqJiDD66vrsTExj/rYjAOw+AYn9P4FOD8HKT+DtVvBmM6vLKDfHzXegVNnl0sFiEbEDcUBD4H1jzNN5jm8A+hpj4p3vdwLtjTFJecqNAcYAhIeHt927d6/LYlalR3aug2tenUdVXy+q+HqyZGcy/t4e/PvmlgyscQyzZzHHN/1B1b2/QeO+MGgiePm6O2ylSiW3LTpnjMk1xkQBYUA7EYnMG1t+p+VTzwRjTIwxJiY0NNQVoapSyNNu4+7O9diUcILdSWk82bsxjav78/CU1dz7ezo9FzWmzdYRvOgYhdk2G764AVIOnK0gPhY+6QVrvnbfTShVBpTIU0PGmOMiMh/oC2w451A8UAeIFxEPoDJwtCRiUmXD3V3q0Sa8Cq3rVMHTbuPerg14c842JizYRZs6VRh5Yz3enuuFeIbwfMJbyDttIOZu8A6AhW8ABg5tgNoxENrY3bejVKnkyqeGQoFsZxKoBPTiwsHgn4A7gaXAIOBPU9YmNiiXstuEmIigM+897Tae7tuUJ3s3wW6zGpQRwb6M+CwLj8gvedb/Z1gxAUyuNRv56ifgs94wYwzcM0eXtVAqH67sGqoJzBORdcBKYI4xZqaIjBORgc4ynwLBIrIDeBwY68J4VDlyOgkAXN0olL91b8jH63Oov6wfXTNe4zGfcWzq8JrVChjwNhxcDX/pQ2lK5UdnFqtyISfXweTl+0hKzQTg29h4UtKzeWtwFH0ja8CM+2HtFGs+Qqvb3BytUiVPZxarcs/DbuPOThFn3g/vUJfRX8Zx31dxjLuhBSOuewNS9sOMe8Fmh8hb3BesUqWM7lCmyqVqgT58M6YD1zavzvM/buTHTcfg9m+gTgf4bjR8fj2ON5qT+e+65K6d5u5wlXIrTQSq3PLxtPPu0Da0rxfEE9PW8teeU3DHNGh2PSY7nRWmGRszQrDPGA1zngdHLg6HYd7WI+Q6ylaXqVJXQhOBKtd8PO18fGcMjaoHcO+Xsfyx6xTc9gXT23zOkKS7Gck/meHRDxa/DTPu47NFuxg5cSXfxcW7O3SlSowmAlXuBfp48uU97WhULYDRX8Ty7h/b+efPm2hXL4j/3taWx1KHs6nxg7B+Gpvnfg7AtNj97g1aqRKkiUBVCCH+3kwd04GrG4XyxpxtALxxa2v6tKhBs5qBPHSgJzu8m/EP+ZT72/oRu/cYOxNTL1KrUuWDJgJVYfh5e/DJnTE82qsR793ehjpBvogID/doyM7kDEafGIW/PZdHT71La/tuls7/BbbNhtjPYNFbcOKgu29BKZfQeQSqwnM4DDe8vxibTfg+ej322U/nX7BqPRg5CwJrlWyAShUDnUegVCFsNuHb+zpiE8Fu7wQ1WhC3fR9vz9/LI/3b0rZlCzi+HybfCpMGwF2/gF81OJUM22fDhu8gJR5G/gZ+wRe/oFKljLYIlMpHVo6DDv/5gxB/L65uFEr1QG+GVD9I4PTB4MiG3GzOLJRbJdxKBB0egD4vuzVupQqiLQKlLpGXh42HezTkk0W7mbJiH6eycvmpdiDfDJ2B37YfOGF8+HlrOhFtutP5mt7w44Ow4mNofx9UqePu8JW6JNoiUKoI5m05wugvYomJqMrj1zbhgcmrSErNJNjPi3lPdSMw4xC8Gw0tb4Mb33d3uEpdwG0b0yhVXnRvWo3Xb23Nsl1Hue2jpfh42nhrcGuOnsri/T93WK2Aq0bD2q/hyBZW7zvGf3/bQkZ2rrtDV+qitGtIqSK6sU1tMrJzmbf1CC/d2JLQAG8W70hm4uI93NG+LuFXP4FZ9QXJn9zCc6kPsNERQdMaAdwQVdvdoStVKO0aUuoKHD6RQffX59O0RgABPp7k7FrEG/Z3CLGl8qUMoIF/Ntd474CarWHgu+Dh5e6QVQXllq4hEakjIvNEZLOIbBSRR/IpU1lEfhaRtc4yI10Vj1KuUD3Qhwe7N2TVvuPsP3aKxu37kHLnfDwaX8tIx/dEp8wl16cqrJtq7ZLm0K4iVfq4smsoB3jCGLNKRAKAOBGZY4zZdE6ZB4FNxpgBzq0tt4rIZGNMlgvjUqpYPdCtAUOuqkOwv/fZD+t9Tey69Qyesof32l1Fv2bT4ffnwMvf2jHNZndbvErl5bJEYIxJABKcr0+KyGagNnBuIjBAgIgI4I+1cX2Oq2JSyhVE5PwkYH1IVGQkgb6H+HXDIfoNfQgyTsCCV2H3Amg3BsJi4EAcHFpvzVqu3xVqt9V9lVWJK5HBYhGJANoAy/Mceg9rA/uDQAAw2BjjKImYlHI1D7uN3s1r8Mv6BDJzcvHu/gzUbIVZ+j7y+7NnC/pVg7REmP9vqBRkzUVoNxp8g9wXvKpQXP74qIj4A98BjxpjTuQ53AdYA9QCooD3RCQwnzrGiEisiMQmJia6OmSlik3fljVIzcxh0fYkECGn8XU85PNvrsv8N2OyHmPaNXPgqe3wf7vgti8gvIOVEMa3tCaoKVUCXPrUkIh4AjOB2caYN/M5/gvwijFmofP9n8BYY8yKgurUp4ZUWZKV46DtS3OoHuhDl4Yh7E1OY97WRJ7p35SF25NYs+84fzzRlWqBPmdPOrwRx+//wLbzD2tuQt9XwO4BaUlQqaqOL6jL4pYlJpz9/p8Cm/NLAk77gJ7AQhGpDjQBdrkqJqVKmpeHjUd7NWbayv1Mi91PZo6D565rxqir63Nt8xr0Gb+Al37ZzEs3RbJ0ZzJLdyazcs9RtiaM5NsGtWiz8mPYswgyjsPJBKjeEoZ/D/7V3H1rqhxxWYtARLoAC4H1wOl+/2eAcABjzIciUgv4HKgJCFbr4KvC6tUWgSqrjDFk5jjw8Tz7jf6tOdt4+4/t2G1CrsPg42mjTZ2qHDqRAcCfPfYhKz+FkEYQ1ACWvAMBNWD4D1C1rrtuRZVBbmkRGGMWYf1xL6zMQaC3q2JQqjQRkfOSAMD93Rpw+EQGIf7edGkUQpvwKnh72Jm8fC/PztjA5ho30vzeEWfKZ0Z0wz7lNrI/6I5X4x7YgxtYTx/V7251H4G1gY4xUFlnNKui0SUmlHIjH087r9zS6oLP+7SowT9+2MCs9Qk0rxWIMYZnZmzgu7jj1HM8w989phC9fRGBG6YDBvxrQNP+kLDWeiTV5gFdx0KXx6wEkbjNejKpbieQQr+fqQpIE4FSpVCIvzcd6gcza30CT/RuzNzNR5iyYh83RtXihqi2fLX8Ku7bkcich9pTJ2kRrJkMq76AGi2h5/NweBPMewk2/wSOHDjinL4T1g56vwTh7d17g6pU0USgVCnVv2VNnvthA+sPpPDyL5toWM2f125tjafdRuMaAfR64y/G/baLj0cMhOYDOZaaAWLDx9OOj6cNaXodzH0BAsOg36vWRLX5/4XPekPDa6HjA1aXkrYQKjxddE6pUiopNZN2L8+ldtVK7D+azucjr6Jbk7NPC334105e+XULt7cPZ138cTYcODtNJ6ZuVaaM6YCnPc9Uoaw0WPYBLP8I0o5AtRbQ60VodK0mhHJO9yNQqgwK8femfb1g9h9Np3uT0POSAMDdnevRpHoAXy/fh6fdxlN9mvDCgObc3bkesXuPMXXFvgsr9fKDa56ExzbAjR9ATgZ8fSt8eZO11IWqkLRrSKlSbFDbMNbsP85z1ze/4JiXh41p93YkMyf3vAlpxhg2HkzhrbnbuaFNbQJ9PFm0PYnPl+xm39FTJBzP4G89GnJv19shchDEfgrzX4EPu0CLm6Hr0xDSGGw2ay/mTT9a6yN1Gwu12pTk7asSol1DSpVixhgysh1U8rq02cTr41MY8N4i7u/WgLpBvjz7wwaqB3jTonZl4o+lE3/0FIue7kFlX+cCd+nHYMl7VrdRdpr1mVcAZJ20Xnv4gE9lGDUXqoQX4x2qklJY15AmAqXKqce+WcNPaw+S6zBc0ziU/90Rjb+3B5sOnqD/Owt5uGcjHr+28fknpSXBhu/gVLK1Wqp/KDS/EXKz4dPe1tyEu3+zkgLAsT2w9TfISYcOD+rGO6WYWyaUKaXc66k+TViwLZHeLWow7oYWZwaOm9cKpG+LGkxctJt7OtfD29PGB/N3AjCiY12C29+bf4WDv4CvboH/dQKfQMhKhePnjEPsWw63TQIP7/zPz06H7b9D436aMEoZbREoVY7lOgx224VPA51uFQxqG8a6+ONsO5yKCHh72BjaLpyn+za9YBY0AJt/htWTrUlqdm+oHQ2N+8Ku+fDL49CgJ1z/ljUo7R1wNilknoQpQ2HPQoi5B64/Z/mxk4ettZP0qSWX0haBUhVUfkkArFZBnxbVmR4XT4i/N5PubkftKj58+NcuJi7egyA8P+D8AeqUU9nsC7iGwD49qBvsd36FwQ3A7gU/PQRvO2dK2zyh2fXQagj89V9r1nPDXtbgdFgMtBoMf74Ei96Ejn+DPi+74legikATgVIV1HPXNSci2I9RV9cnNMD65v76ra3x87Lz2eLd9GxWjc4NQ1iwLZGnpq/l8IlMAAK8PS5cOhsgejiENoXEzZCdAck7YN03sHGG1XoYMtmayPbVTTDzMVj9FexdbM1lWPoeVA6DDveX9K9BoV1DSqk80rNyue7dhaRn5TKiYwSvzd5C4+oBDGobRlVfL/7+/Xr6t6zB+CFFeJQ0OwO2/gJVIiCsrfVZWhJ81NWa0Nb/dWgzDKaNgC2/WGMMzW9w6f1VVPrUkFLqkqzdf5ybP1hCrsPQu3l13hochZ+31YHw5u9beefPHUwd04EO9YPPnJPrMCSkpFMtwAcvj4vMVT1x0Bo8Dm5gvc9Oh0kDID4WOjwAPZ4DL9+z5Y/vh1+ftjblaT3UmgmteztfEk0ESqlL9l1cPMlpmYzqUh/bOWMN6Vm5XPvWX1TytPN036bM33aE2D3H2JWURlaOg+ta1eT926Mv/YKZJ2HOC9YYQtUIaH8/NOgOx/bCjDGQm2MNPp9KsvZ57veKNQFOB5mLRBOBUqpYzdl0mNFfWP8/9PWy065eEE2qB5CclsX0uHg+HBZN38iaZ8qfzMhmWmw8f21L5OUbI6kT5FtQ1daObLOeOrtiKlirqt46yZrMtvNPayb0wVXQbCBc96Y13wHA4YC/XrFWXO35vCtuvcxySyIQkTrAF0ANrB3KJhhj3s6nXDdgPOAJJBljuhZWryYCpUqHn9YeJMjXi6vqVcXbw3rUNDvXwY3vL+bwiUzmPn4NgvDBXzv5atleUjNzsAlc27w6Hw3P9+/R+Y7tgV1/QeYJuGoUeFY6eyw3x9qtbf5/wDsQbnjPGoj+6SFY+7VV5m+x1s5uCnBfIqgJ1DTGrBKRACAOuNEYs+mcMlWAJUBfY8w+EalmjDlSWL2aCJQq3TYeTOGG9xbTKqwyu5LSSEnP5vpWtRjVpR6LdiTx2uytTB7Vns4NQ86ck5XjYNW+Y3jYhJiIoKJf7PAm+H4MHF5vrY+UtM16FHXFBIgeAde94YI7LJvcsvqoMSbBGLPK+foksBnIu3fe7cD3xph9znKFJgGlVOnXolZl7u/WgFX7jhNZqzK/PHQ17w5tQ+s6VbinSz3Cg3z5588bycl1ELvnKKO/iCVq3O8MmbCMoR8vY1/yqXzrXbwjif1H8xyr3hxG/wGdH7FaEH1fseYjtLwN1nwNp45a5RLWwdqpBQeddcra3rOCKpExAhGJABYAkcaYE+d8frpLqAUQALxtjPkin/PHAGMAwsPD2+7du9flMSulLp/DYdidnEb9ED8kz2Du7I2HuPfLOBqE+rEzMY1gPy/6taxBTN0g/v79eno0q3bBYPMH83fy39+2UCeoEj//rQtVfPNZoiI3++yTRIc2wIedrb0W6rSHybdaS2L0exVOL6Gx8QdrMtuxvZBxHOp2hls/t2Y5l0NuHSwWEX/gL+BlY8z3eY69B8QAPYFKwFLgOmPMtoLq064hpco2Ywz3TIpl7f7j3Nu1PsM61MXXy/lo6pxtvPPHdr5/oBPR4VUxxvDq7K18MH8n1zQOZenOJDo3DOGzO68670mmfE0aYHUdZadDYE2oWg92/gGDJ1uT3v4YZ01mC+9grZ207EOoVBWGfAW12+ZfZ/JO6+mmWlHF/FtxPbclAhHxBGYCs40xb+ZzfCzgY4x50fn+U+A3Y8y3BdWpiUCpsi8n1wGAR54d1NIyc+j62nzqBvvyRO/GvD13O8t3H+WO9uGMuyGSKSv28dwPG3ikZyMey7NyanJqJsH+5yx4t/U3mDIYQprAnT9Zax99fp211IVxWHsx3PA+eDpnSCesg6l3QOohaHsXdHkMAmudrW/1V/DLk9YTSUOnWHMZyhB3DRYLMAk4aox5tIAyzYD3gD6AF7ACGGKM2VBQvZoIlCrfvl6+j2dmWLulVQvw5qEeDRnWoS4igjGGJ75dy/erDtC7eXWe7NOEtMwcXv99K4t3JPPpnTH0bFbdqsgYcjfMYK1HS+buzcUmwhOdKiPfDLMWx+s29sI5CGnJ1j7Pa6eA2Kw9nauEc+r4IXy3/wwRV0NGijUofcd0qHd1Cf92Lp+7EkEXYCGwHuvxUYBngHAAY8yHznJPASOdZT4xxowvrF5NBEqVbzm5Dv41cxP1QvwY0i78glVQM3Ny+eivXXy8YBepWTkYA8F+XthsQvVAb37+WxdEhP1HTzHowyUcPpGJiDUW/OKA5tzVud4F1zyaloWvl/3stY7thcXjYf8Kso/ug6w0DkTeR8Qt/4L041bL4vg+6PgANOgBYVcVPNP59N9YN0980wllSqly51haFpOW7sHbw87wjnWZtS6B//tuHZ/dFUP3JtW4c+JKYvcc5dVBrbi6USiPf7OGhTuS+PHBzjSrGXimnrX7j3P7x8vo2aw67wy9cP2k2z5aSuzuJAZGhZ1dX+nkIfh+tDX5zTjA089aUTW8I7QbDX7OR2MdufBxD2vLz7AYK2E07AU1W5d4YtBEoJQq97JzHXR7bT6hAd6M6FiXx6etPa8FkJyaSb+3FxJYyZMfH+yMn7cH2w+f5NaPlpKSno1NhEVPd6dm5bMT1+L2HuWWD5ZS1deT7FxD3D96nZk8B1itg90LrJ/9y6ynlRr2hGHfWcfXT4fv7rEmux3fa3UpAQTUtJbH6PSQNZBdAjQRKKUqhNPjC94eNprXCmT6fZ3O25Nh0fYkhn+2HB8POzERVdl2+CQOA+MHRzHs0+U82K0hT/Zpcqb8qEkrid17jP/c1JL7J686fwwiP0veg9+fhTu+s9ZJ+l9Ha6G8+xaDzQapibBjDmydBVtmORfRGwJB9UHs1tNI9a5xye/GLRPKlFKqpN3Stja1KvvgMIb/3tLqgo15ujQK4etRHRh8VR2OnMjE28POl/e0o3PDEHo2rc6UFfvIyM4FYOuhk8zdfIS7OkXQs1l1An08+GV9QuEBtBttPab6+3Ow4XtI2grXPGUlAbDWRIq6HQZ/BQ/FWa/XToW5L8Kcf8CkgbBmigt+M4XTjWmUUuWGt4edCSNiOHYqi8bVA/It07FBMB0bBF/w+V2dIpi7+TCz1ifQtXEo42ZuxNfLzp0dI/DysNG7RQ1mbzxEVo7jvGW29yWfYk9yGtc0DrVWR712HEwbbq17FNqU7KYDsTvMhfMegurBgLeh/xvgyLbmO0wfCT84N+epEg5rJsO+pZCZCllp1uB0j+eK7fd1miYCpVS5Elm78mWd17lhMA1C/Xhzzjb++fMm0jJzeH5Ac6r6WbOY+7eswfS4eBbvTKJ7E2v28YYDKYz4bAXHTmXx15PdCQ/2hWYDILwT7FtCylWPcP2bC+jSMJT/3Nwy3+tuOJRGi1qBiGclGDLFmvvww33WQa8Aq4vJNwi8/K1Z0i6giUAppQARYWTnejz3wwba1QvipRsjz2tVdG4YQoCPB5OW7CHQx5OsHAf3fhmLn7cHdhEmLd3DP65vbj0NNPBdsldPYcSy2uw/mso3K/cx+up61A/1P++aP689yENTVvPqoFbcFlPH2oxn6DfWHs/VmllJxSvP/tAuoGMESinldEf7cGY+1IVvxnS4oGvJ28PO7e3Cmb81kVs+WMLQj5dRxdeLb+/ryHWtavLNyv2czMgGwBHUgAcT+rE+IZVXB7XCy8PGe/N2nFdfRnYur/y6BYDPFu3mzIM7Xr5w7T+tQeQSSAKgiUAppc4QESJrV75gobzTxvZryqKnu/PJiBheHNCc6fd1JKyqLyM71yM1M4fpcfE4HIZnf1jP75sO84/rm3NbTB2Gta/Lj2sOsicp7UxdExbs4sDxdG6JDmPLoZMs23W0pG7zAto1pJRSRSQihFX1Jazq+TusRdWpQtu6VZm4eA/r41P4fvUBHuzegJHOOQxjutbny2V7eX/eDl67tTWHUjL4YP5O+kXW4OWbIvlzy2E+X7Kbjg2C2Xb4JI9MXYOHTWhaI4AujUK4ISrvCv7FSxOBUkoVg7s71+PBr1ex7+gpnri2MQ/1PLs7WrUAH25vH87nS/awcHsS2bkOch2GZ/o3w8fTztB24Xz4107+3HKYp75dh82ZBOZtPcK3cfHsSz51Xn3FTROBUkoVgz4tqtO7eXU6Ngg+0xI416O9GlPJ005SaiansnLp1qTamb2bh3esy0cLdnH357FUC/Bm6pgO1A/1x+EwPPntWt6Ysw1vTxtjrmngktg1ESilVDHwsNuYMKLgvZgrV/Lk//o2zfdYzcqVuCW6Ngu2JTF5dPszTxfZbMKrg1qRlevg37O24O1h585OEcUfe7HXqJRS6pL95+ZWOIzBM88eDR52G28NjsImQniQbwFnXxlNBEopVQrYbYKd/J9W8rTb8l0Ztbjo46NKKVXBaSJQSqkKzmWJQETqiMg8EdksIhtF5JFCyl4lIrkiMshV8SillMqfK8cIcoAnjDGrRCQAiBOROcaYTecWEhE78F9gtgtjUUopVQCXtQiMMQnGmFXO1yeBzUB+0+MeAr4DjrgqFqWUUgUrkTECEYkA2gDL83xeG7gJ+PAi548RkVgRiU1MTHRVmEopVSG5PBGIiD/WN/5HjTEn8hweDzxtjMktrA5jzARjTIwxJiY0NNRVoSqlVIXk0nkEIuKJlQQmG2O+z6dIDDDVudJfCNBfRHKMMT+4Mi6llFJnuWzzerH+uk8CjhpjHi1C+c+BmcaY6RcplwjsvcywQoCkyzy3NCpP96P3UjrpvZROl3MvdY0x+XapuLJF0BkYDqwXkTXOz54BwgGMMYWOCxSkoBspChGJNcYUvBhIGVOe7kfvpXTSeymdivteXJYIjDGLoID50vmXv8tVsSillCqYzixWSqkKrqIlggnuDqCYlaf70XspnfReSqdivReXDRYrpZQqGypai0AppVQemgiUUqqCqzCJQET6ishWEdkhImPdHc+lKGglVxEJEpE5IrLd+b9V3R1rUYmIXURWi8hM5/t6IrLceS/fiIiXu2MsChGpIiLTRWSL89+nY1n9dxGRx5z/fW0QkSki4lOW/l1E5DMROSIiG875LN9/C7G84/x7sE5Eot0X+YUKuJfXnP+drfv/9u43RKoqjOP490FrWQ3ZMhJrqzVaKpW0EBGLCAtyTdygF64ISQmBBPqqbNlXQW+E6I9kRmmJIQlFf/ZNomxRRJkl2P+sTZfc0lJKoz+o2a8X50xdxhmdm+7ePdznA5e598zs7Dk8w33mnjv3uWb2qpm1ZJ7rjmPZbWa35f1/pUgEscLpGqADmAwsMrPJxfYql0ol12uAWcB9sf8PAn2S2oG+uJ2KFYRChBWrgMfiWH4BlhbSq/yeALZIuhqYRhhTcnGJdb+WAzMkTQVGAV2kFZcNwNyqtnqx6ADa43IvsHaY+tioDZw8lm3AVEnXAl8D3QBxX9AFTIl/81Tc5zWsFIkAmAn0S9oj6RiwGegsuE8NO0Ul107C1dvExzuK6WE+ZtYK3A6si9sGzAEqV5UnMRYzGwfcBKwHkHRM0mESjQvhuqJmMxsNjAH2k1BcJL0D/FzVXC8WncBGBduBFjObODw9Pb1aY5G0VdJfcXM70BrXO4HNko5K2gv0E/Z5DStLIrgE2JfZHqR2SewRr6qS6wRJ+yEkC+Ci4nqWy+PAA8DfcXs8cDjzIU8lPlcAB4Hn4zTXOjMbS4JxkfQ98AjwHSEBHAF2kmZcsurFIvV9wj3AG3H9jMdSlkRQ6wrn5H43e5pKrkkws/nAT5J2ZptrvDSF+IwGrgfWSroO+J0EpoFqiXPnncAk4GJgLGH6pFoKcWlEqp85zKyHMF28qdJU42W5xlKWRDAIXJrZbgV+KKgv/0udSq4/Vg5n42MKN/e5AVhgZgOEKbo5hCOEljglAenEZxAYlFS5z8bLhMSQYlxuBfZKOijpOPAKMJs045JVLxZJ7hPMbAkwH1is/y4CO+OxlCURfAi0x19A8W1HogAAAqhJREFUnEs4sdJbcJ8aFufQ1wNfSno081QvsCSuLwFeH+6+5SWpW1KrpDZCHN6UtBh4C6jcszqVsRwA9pnZVbHpFuALEowLYUpolpmNiZ+3yliSi0uVerHoBe6Kvx6aBRypTCGNVGY2F1gJLJD0R+apXqDLzJrMbBLhBPiOXG8uqRQLMI9wpv1boKfo/uTs+42EQ71PgF1xmUeYW+8DvomPFxTd15zjuplQehzCfPsOwomul4CmovvX4BimAx/F2LwGnJ9qXICHgK+Az4AXgKaU4gK8SDi/cZzwLXlpvVgQplPWxP3Bp4RfSxU+htOMpZ9wLqCyD3g68/qeOJbdQEfe/+clJpxzruTKMjXknHOuDk8EzjlXcp4InHOu5DwROOdcyXkicM65kvNE4FwVMzthZrsyy1m7WtjM2rIVJZ0bCYbs5vXOJexPSdOL7oRzw8WPCJxrkJkNmNkqM9sRlytj++Vm1hfrxPeZ2WWxfUKsG/9xXGbHtxplZs/G2v9bzay5sEE5hycC52pprpoaWph57ldJM4EnCTWSiOsbFerEbwJWx/bVwNuSphFqEH0e29uBNZKmAIeBO4d4PM6dkl9Z7FwVM/tN0nk12geAOZL2xCKABySNN7NDwERJx2P7fkkXmtlBoFXS0cx7tAHbFG6UgpmtBM6R9PDQj8y52vyIwLl8VGe93mtqOZpZP4Gfq3MF80TgXD4LM4/vx/X3CJVUARYD78b1PmAZ/HuP5nHD1Unn8vBvIs6drNnMdmW2t0iq/IS0ycw+IHyJWhTblgPPmdn9hDuW3R3bVwDPmNlSwjf/ZYSKks6NKH6OwLkGxXMEMyQdKrovzp1NPjXknHMl50cEzjlXcn5E4JxzJeeJwDnnSs4TgXPOlZwnAuecKzlPBM45V3L/AAAO9n8afbmTAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(loss_list,label='Train')\n",
    "plt.plot(loss_val_list,label='Validation')\n",
    "\n",
    "plt.title('Train and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T1 - Save the model \n",
    "After training, save the model so we can load it again later if we need too. Here I'm saving the parameters needed to create the same architecture, the hidden layer hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the name, for saving multiple files\n",
    "model_name = 'LSTM_T1.net'\n",
    "\n",
    "checkpoint = {'n_hidden': net1.n_hidden,\n",
    "              'n_layers': net1.n_layers,\n",
    "              'state_dict': net1.state_dict(),\n",
    "              'tokens': net1.chars}\n",
    "\n",
    "with open(model_name, 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T1 - Sampling\n",
    "\n",
    "Now that the model is trained, we'll want to sample from it. To sample, we pass in a character and have the network predict the next character. Then we take that character, pass it back in, and get another predicted character. Given the **input (size)** in the function **generate_name**, you can generate a given sized name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_name(net, size, prime, top_k=None, cuda=False):\n",
    "        \n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "\n",
    "    net.eval()\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    \n",
    "    h = net.init_hidden(1)\n",
    "    \n",
    "    for ch in prime:\n",
    "        char, h = net.predict(ch, h, cuda=cuda, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        #print(ii)\n",
    "        char, h = net.predict(chars[-1], h, cuda=cuda, top_k=top_k)\n",
    "        chars.append(char)\n",
    "        #print(chars)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alile\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generate_name(net1, 4, prime='A', top_k=5, cuda=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T1 - Load_model_T1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we have loaded in a model that trained and shared on drive, 'LSTM_T1.net'\n",
    "\n",
    "#https://drive.google.com/file/d/105D2knhyMdowATI8IVCndfNrHvrBCJYk/view?usp=sharing\n",
    "fid = '105D2knhyMdowATI8IVCndfNrHvrBCJYk'\n",
    "\n",
    "import wget\n",
    "def download(fid, fn):\n",
    "    durl = 'https://drive.google.com/' + 'uc?export=download&id=' + fid\n",
    "    print('downloading from', durl)\n",
    "    wget.download(durl, fn)\n",
    "\n",
    "def load_model_T1():\n",
    "    download(fid, 'LSTM_T1.net')\n",
    "    with open('LSTM_T1.net', 'rb') as f:\n",
    "         checkpoint = torch.load(f)\n",
    "    Loaded_model = Gene_LSTM(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
    "    Loaded_model.load_state_dict(checkpoint['state_dict'])\n",
    "    \n",
    "    return Loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading from https://drive.google.com/uc?export=download&id=105D2knhyMdowATI8IVCndfNrHvrBCJYk\n",
      "Arrin\n",
      "Allane\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "# Change cuda to True if you are using GPU!\n",
    "loaded = load_model_T1()\n",
    "print(generate_name(loaded, 10, cuda=False, top_k=5, prime=\"A\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T2 - Generate fake names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fake_name(length):\n",
    "    name_letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    name_len = length-1 #random.randint(0,26)\n",
    "    name_letter = random.sample(name_letters,name_len)\n",
    "    name_letter.insert(0,name_letter[0].upper())\n",
    "    fake_name = reduce(lambda x,y:x+y,name_letter)\n",
    "    return str(fake_name)+'\\n'\n",
    "fake_names=''\n",
    "real_names=''\n",
    "\n",
    "with open('yob2018-small.txt', 'r') as fp:\n",
    "    real_names=fp.read()\n",
    "    \n",
    "with open('yob2018-small.txt', 'r') as fp:   \n",
    "    line = fp.readline()\n",
    "    cnt = 1\n",
    "    while line:\n",
    "        #print(\"Line {}: {}\".format(cnt, line.strip()))\n",
    "        fake_names+=generate_fake_name(len(line.strip()))\n",
    "        line = fp.readline()\n",
    "        cnt += 1            \n",
    "\n",
    "category_lines = {'Real':real_names,'Fake':fake_names}\n",
    "all_categories = ['Real','Fake']\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "n_categories = len(all_categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indices_encode(arr, n_labels):\n",
    "    \n",
    "    # Initialize the the encoded array\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    \n",
    "    # Fill the appropriate elements with ones\n",
    "    for i in range(one_hot.shape[0]):\n",
    "        one_hot[i]=arr.flatten()[i]\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    #print(one_hot.shape)\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the text and map each character to an integer and vice versa\n",
    "\n",
    "# we create two dictonaries:\n",
    "# 1. int2char, which maps integers to characters\n",
    "# 2. char2int, which maps characters to unique integers\n",
    "\n",
    "chars = tuple(set(real_names))\n",
    "\n",
    "int2char = dict(enumerate(chars))\n",
    "\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "\n",
    "encoded1 = np.array([ord(ch)-ord('A') for ch in real_names])\n",
    "encoded2=np.array([ord(ch)-ord('A') for ch in fake_names])\n",
    "encoded=np.append(encoded1, encoded2)\n",
    "target1= np.array([1 for ch in real_names])\n",
    "target2=np.array([0 for ch in fake_names])\n",
    "target=np.append(target1, target2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr,label,n_seqs, n_steps):\n",
    "    ''''Create a generator that returns batches of size\n",
    "       n_seqs x n_steps from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       n_seqs: Batch size, the number of sequences per batch\n",
    "       n_steps: Number of sequence steps per batch\n",
    "    '''\n",
    "    \n",
    "    batch_size = n_seqs * n_steps\n",
    "    n_batches = len(arr)//batch_size\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * batch_size]\n",
    "    \n",
    "    # Reshape into n_seqs rows\n",
    "    arr = arr.reshape((n_seqs, -1))\n",
    "    \n",
    "    label = label[:n_batches * batch_size]\n",
    "    \n",
    "    # Reshape into n_seqs rows\n",
    "    label = label.reshape((n_seqs, -1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        \n",
    "        # The features\n",
    "        x = arr[:, n:n+n_steps]\n",
    "        \n",
    "        # The targets, shifted by one\n",
    "        #y = np.zeros_like(x)\n",
    "        y = label[:, n:n+n_steps]\n",
    "        \n",
    "        yield x,y\n",
    "        \n",
    "    #return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T2 - The is_real_name function is defined in Classifier_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier_LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, n_steps=100, n_hidden=128, n_layers=1,\n",
    "                       drop_prob=0.5, lr=0.0001):   #  \n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        # Creating character dictionaries\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        ## Define the LSTM\n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
    "             dropout=drop_prob, batch_first=True)   # \n",
    "        \n",
    "        ## Define a dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        ## Define the final, fully-connected output layer\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "        \n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.h=self.init_hidden(1)\n",
    "        # Initialize the weights\n",
    "        self.init_weights()\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hc):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hc`. '''\n",
    "        \n",
    "        ## Get x, and the new hidden state (h, c) from the lstm\n",
    "        x, (h, c) = self.lstm(x, hc)\n",
    "        \n",
    "        ## Ppass x through the dropout layer\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Stack up LSTM outputs using view\n",
    "        #x = x.view(x.size()[0]*x.size()[1], self.n_hidden)\n",
    "        x = x.reshape(x.size()[0]*x.size()[1], self.n_hidden)\n",
    "\n",
    "        x=self.sig(x)\n",
    "\n",
    "        # Return x and the hidden state (h, c)\n",
    "        return x, (h, c)\n",
    "    \n",
    "    \n",
    "    def init_weights(self):\n",
    "        ''' Initialize weights for fully connected layer '''\n",
    "        initrange = 0.1\n",
    "        \n",
    "        # Set bias tensor to all zeros\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        # FC weights as random uniform\n",
    "        self.fc.weight.data.uniform_(-1, 1)\n",
    "        \n",
    "    def init_hidden(self, n_seqs):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x n_seqs x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        return (weight.new(self.n_layers, n_seqs, self.n_hidden).zero_(),\n",
    "                weight.new(self.n_layers, n_seqs, self.n_hidden).zero_())\n",
    "    \n",
    "    def is_real_name(self, name):\n",
    "        h = self.init_hidden(1)      \n",
    "        p = 0   \n",
    "        for char in name:\n",
    "            x = np.array([[ord(char)-ord('A')]])\n",
    "            x = indices_encode(x, len(self.chars))\n",
    "            inputs = torch.from_numpy(x)           \n",
    "            h = tuple([each.data for each in h])\n",
    "            out, h = self.forward(inputs,h)\n",
    "            p = torch.sigmoid(out).data\n",
    "                \n",
    "       # print(\"SIGMOID value is \"+str(torch.mean(p, dim=1)))\n",
    "        p = torch.mean(p, dim=1)\n",
    "        if p>0.5:\n",
    "            print(\"This is a real name\")\n",
    "        else:\n",
    "            print(\"This is a fake name\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_val_list = []\n",
    "loss_list = []\n",
    "def train(net, data, label, epochs=10, n_seqs=10, n_steps=50, lr=0.001, clip=5, val_frac=0.1, cuda=False, print_every=10):\n",
    "    ''' Training a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: Gene_LSTM network\n",
    "        data: text data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        n_seqs: Number of mini-sequences per mini-batch, aka batch size\n",
    "        n_steps: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        cuda: Train with CUDA on a GPU\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    net.train()\n",
    "    \n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    label,val_label=label[:val_idx], label[val_idx:]\n",
    "    \n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        \n",
    "        h = net.init_hidden(n_seqs)\n",
    "        \n",
    "        for x, y in get_batches(data, label, n_seqs, n_steps):\n",
    "            \n",
    "            counter += 1\n",
    "            x = indices_encode(x,n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            if cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            net.zero_grad()\n",
    "            \n",
    "            output, h = net.forward(inputs, h)\n",
    "            \n",
    "            net.h=h\n",
    "            loss = criterion(output, targets.reshape(n_seqs*n_steps).type(torch.LongTensor))\n",
    "            loss.backward()\n",
    "            \n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "\n",
    "            opt.step()\n",
    "            \n",
    "            if counter % print_every == 0:\n",
    "                \n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(n_seqs)\n",
    "                val_losses = []\n",
    "    \n",
    "                for x, y in get_batches(val_data,val_label, n_seqs, n_steps):\n",
    "                    x = indices_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "                    if cuda:\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net.forward(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.reshape(n_seqs*n_steps).type(torch.LongTensor))\n",
    "                    val_losses.append(val_loss.item())\n",
    "                    \n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n",
    "                \n",
    "                loss_val_list.append(np.mean(val_losses))\n",
    "                loss_list.append(loss.item())\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'net' in locals():\n",
    "    del net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier_LSTM(\n",
      "  (lstm): LSTM(53, 128, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=128, out_features=53, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialize and print the network\n",
    "#net2 = Classifier_LSTM(vocab_size=len(real_names), output_size= len(real_names),n_embedding=320, n_hidden=128, n_layers=1)\n",
    "net2 = Classifier_LSTM(chars, n_hidden=128, n_layers=1)\n",
    "print(net2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100... Step: 10... Loss: 4.6998... Val Loss: 4.6982\n",
      "Epoch: 1/100... Step: 20... Loss: 4.6858... Val Loss: 4.6831\n",
      "Epoch: 2/100... Step: 30... Loss: 4.6682... Val Loss: 4.6687\n",
      "Epoch: 2/100... Step: 40... Loss: 4.6604... Val Loss: 4.6607\n",
      "Epoch: 2/100... Step: 50... Loss: 4.6510... Val Loss: 4.6450\n",
      "Epoch: 3/100... Step: 60... Loss: 4.6199... Val Loss: 4.6178\n",
      "Epoch: 3/100... Step: 70... Loss: 4.6060... Val Loss: 4.6078\n",
      "Epoch: 4/100... Step: 80... Loss: 4.6054... Val Loss: 4.6070\n",
      "Epoch: 4/100... Step: 90... Loss: 4.5952... Val Loss: 4.5911\n",
      "Epoch: 4/100... Step: 100... Loss: 4.5914... Val Loss: 4.5883\n",
      "Epoch: 5/100... Step: 110... Loss: 4.5863... Val Loss: 4.5879\n",
      "Epoch: 5/100... Step: 120... Loss: 4.5911... Val Loss: 4.5802\n",
      "Epoch: 6/100... Step: 130... Loss: 4.5795... Val Loss: 4.5732\n",
      "Epoch: 6/100... Step: 140... Loss: 4.5737... Val Loss: 4.5751\n",
      "Epoch: 6/100... Step: 150... Loss: 4.5716... Val Loss: 4.5703\n",
      "Epoch: 7/100... Step: 160... Loss: 4.5719... Val Loss: 4.5684\n",
      "Epoch: 7/100... Step: 170... Loss: 4.5670... Val Loss: 4.5689\n",
      "Epoch: 8/100... Step: 180... Loss: 4.5650... Val Loss: 4.5660\n",
      "Epoch: 8/100... Step: 190... Loss: 4.5724... Val Loss: 4.5623\n",
      "Epoch: 8/100... Step: 200... Loss: 4.5661... Val Loss: 4.5651\n",
      "Epoch: 9/100... Step: 210... Loss: 4.5668... Val Loss: 4.5614\n",
      "Epoch: 9/100... Step: 220... Loss: 4.5635... Val Loss: 4.5648\n",
      "Epoch: 10/100... Step: 230... Loss: 4.5604... Val Loss: 4.5621\n",
      "Epoch: 10/100... Step: 240... Loss: 4.5648... Val Loss: 4.5629\n",
      "Epoch: 10/100... Step: 250... Loss: 4.5651... Val Loss: 4.5595\n",
      "Epoch: 11/100... Step: 260... Loss: 4.5564... Val Loss: 4.5623\n",
      "Epoch: 11/100... Step: 270... Loss: 4.5596... Val Loss: 4.5575\n",
      "Epoch: 12/100... Step: 280... Loss: 4.5616... Val Loss: 4.5591\n",
      "Epoch: 12/100... Step: 290... Loss: 4.5602... Val Loss: 4.5584\n",
      "Epoch: 12/100... Step: 300... Loss: 4.5617... Val Loss: 4.5564\n",
      "Epoch: 13/100... Step: 310... Loss: 4.5596... Val Loss: 4.5579\n",
      "Epoch: 13/100... Step: 320... Loss: 4.5604... Val Loss: 4.5567\n",
      "Epoch: 14/100... Step: 330... Loss: 4.5564... Val Loss: 4.5574\n",
      "Epoch: 14/100... Step: 340... Loss: 4.5621... Val Loss: 4.5550\n",
      "Epoch: 14/100... Step: 350... Loss: 4.5513... Val Loss: 4.5545\n",
      "Epoch: 15/100... Step: 360... Loss: 4.5544... Val Loss: 4.5552\n",
      "Epoch: 15/100... Step: 370... Loss: 4.5576... Val Loss: 4.5560\n",
      "Epoch: 16/100... Step: 380... Loss: 4.5579... Val Loss: 4.5541\n",
      "Epoch: 16/100... Step: 390... Loss: 4.5576... Val Loss: 4.5559\n",
      "Epoch: 16/100... Step: 400... Loss: 4.5582... Val Loss: 4.5530\n",
      "Epoch: 17/100... Step: 410... Loss: 4.5576... Val Loss: 4.5534\n",
      "Epoch: 17/100... Step: 420... Loss: 4.5562... Val Loss: 4.5566\n",
      "Epoch: 18/100... Step: 430... Loss: 4.5524... Val Loss: 4.5580\n",
      "Epoch: 18/100... Step: 440... Loss: 4.5593... Val Loss: 4.5542\n",
      "Epoch: 18/100... Step: 450... Loss: 4.5600... Val Loss: 4.5563\n",
      "Epoch: 19/100... Step: 460... Loss: 4.5591... Val Loss: 4.5514\n",
      "Epoch: 19/100... Step: 470... Loss: 4.5548... Val Loss: 4.5504\n",
      "Epoch: 20/100... Step: 480... Loss: 4.5560... Val Loss: 4.5516\n",
      "Epoch: 20/100... Step: 490... Loss: 4.5547... Val Loss: 4.5528\n",
      "Epoch: 20/100... Step: 500... Loss: 4.5545... Val Loss: 4.5538\n",
      "Epoch: 21/100... Step: 510... Loss: 4.5570... Val Loss: 4.5537\n",
      "Epoch: 21/100... Step: 520... Loss: 4.5532... Val Loss: 4.5520\n",
      "Epoch: 22/100... Step: 530... Loss: 4.5447... Val Loss: 4.5479\n",
      "Epoch: 22/100... Step: 540... Loss: 4.5544... Val Loss: 4.5532\n",
      "Epoch: 22/100... Step: 550... Loss: 4.5573... Val Loss: 4.5543\n",
      "Epoch: 23/100... Step: 560... Loss: 4.5593... Val Loss: 4.5550\n",
      "Epoch: 23/100... Step: 570... Loss: 4.5540... Val Loss: 4.5537\n",
      "Epoch: 24/100... Step: 580... Loss: 4.5594... Val Loss: 4.5449\n",
      "Epoch: 24/100... Step: 590... Loss: 4.5490... Val Loss: 4.5516\n",
      "Epoch: 24/100... Step: 600... Loss: 4.5491... Val Loss: 4.5511\n",
      "Epoch: 25/100... Step: 610... Loss: 4.5532... Val Loss: 4.5498\n",
      "Epoch: 25/100... Step: 620... Loss: 4.5566... Val Loss: 4.5500\n",
      "Epoch: 26/100... Step: 630... Loss: 4.5543... Val Loss: 4.5508\n",
      "Epoch: 26/100... Step: 640... Loss: 4.5501... Val Loss: 4.5487\n",
      "Epoch: 26/100... Step: 650... Loss: 4.5534... Val Loss: 4.5484\n",
      "Epoch: 27/100... Step: 660... Loss: 4.5498... Val Loss: 4.5503\n",
      "Epoch: 27/100... Step: 670... Loss: 4.5553... Val Loss: 4.5524\n",
      "Epoch: 28/100... Step: 680... Loss: 4.5539... Val Loss: 4.5517\n",
      "Epoch: 28/100... Step: 690... Loss: 4.5510... Val Loss: 4.5501\n",
      "Epoch: 28/100... Step: 700... Loss: 4.5535... Val Loss: 4.5530\n",
      "Epoch: 29/100... Step: 710... Loss: 4.5524... Val Loss: 4.5494\n",
      "Epoch: 29/100... Step: 720... Loss: 4.5483... Val Loss: 4.5462\n",
      "Epoch: 30/100... Step: 730... Loss: 4.5527... Val Loss: 4.5508\n",
      "Epoch: 30/100... Step: 740... Loss: 4.5556... Val Loss: 4.5492\n",
      "Epoch: 30/100... Step: 750... Loss: 4.5462... Val Loss: 4.5475\n",
      "Epoch: 31/100... Step: 760... Loss: 4.5471... Val Loss: 4.5508\n",
      "Epoch: 31/100... Step: 770... Loss: 4.5498... Val Loss: 4.5465\n",
      "Epoch: 32/100... Step: 780... Loss: 4.5501... Val Loss: 4.5489\n",
      "Epoch: 32/100... Step: 790... Loss: 4.5556... Val Loss: 4.5462\n",
      "Epoch: 32/100... Step: 800... Loss: 4.5475... Val Loss: 4.5463\n",
      "Epoch: 33/100... Step: 810... Loss: 4.5545... Val Loss: 4.5478\n",
      "Epoch: 33/100... Step: 820... Loss: 4.5544... Val Loss: 4.5446\n",
      "Epoch: 34/100... Step: 830... Loss: 4.5518... Val Loss: 4.5459\n",
      "Epoch: 34/100... Step: 840... Loss: 4.5550... Val Loss: 4.5455\n",
      "Epoch: 34/100... Step: 850... Loss: 4.5510... Val Loss: 4.5484\n",
      "Epoch: 35/100... Step: 860... Loss: 4.5489... Val Loss: 4.5510\n",
      "Epoch: 35/100... Step: 870... Loss: 4.5521... Val Loss: 4.5494\n",
      "Epoch: 36/100... Step: 880... Loss: 4.5493... Val Loss: 4.5493\n",
      "Epoch: 36/100... Step: 890... Loss: 4.5524... Val Loss: 4.5503\n",
      "Epoch: 36/100... Step: 900... Loss: 4.5515... Val Loss: 4.5518\n",
      "Epoch: 37/100... Step: 910... Loss: 4.5494... Val Loss: 4.5450\n",
      "Epoch: 37/100... Step: 920... Loss: 4.5487... Val Loss: 4.5449\n",
      "Epoch: 38/100... Step: 930... Loss: 4.5517... Val Loss: 4.5460\n",
      "Epoch: 38/100... Step: 940... Loss: 4.5466... Val Loss: 4.5475\n",
      "Epoch: 38/100... Step: 950... Loss: 4.5508... Val Loss: 4.5485\n",
      "Epoch: 39/100... Step: 960... Loss: 4.5496... Val Loss: 4.5485\n",
      "Epoch: 39/100... Step: 970... Loss: 4.5491... Val Loss: 4.5471\n",
      "Epoch: 40/100... Step: 980... Loss: 4.5527... Val Loss: 4.5461\n",
      "Epoch: 40/100... Step: 990... Loss: 4.5534... Val Loss: 4.5476\n",
      "Epoch: 40/100... Step: 1000... Loss: 4.5531... Val Loss: 4.5494\n",
      "Epoch: 41/100... Step: 1010... Loss: 4.5519... Val Loss: 4.5480\n",
      "Epoch: 41/100... Step: 1020... Loss: 4.5501... Val Loss: 4.5498\n",
      "Epoch: 42/100... Step: 1030... Loss: 4.5469... Val Loss: 4.5495\n",
      "Epoch: 42/100... Step: 1040... Loss: 4.5535... Val Loss: 4.5484\n",
      "Epoch: 42/100... Step: 1050... Loss: 4.5545... Val Loss: 4.5465\n",
      "Epoch: 43/100... Step: 1060... Loss: 4.5474... Val Loss: 4.5536\n",
      "Epoch: 43/100... Step: 1070... Loss: 4.5460... Val Loss: 4.5453\n",
      "Epoch: 44/100... Step: 1080... Loss: 4.5498... Val Loss: 4.5523\n",
      "Epoch: 44/100... Step: 1090... Loss: 4.5531... Val Loss: 4.5483\n",
      "Epoch: 44/100... Step: 1100... Loss: 4.5492... Val Loss: 4.5499\n",
      "Epoch: 45/100... Step: 1110... Loss: 4.5456... Val Loss: 4.5468\n",
      "Epoch: 45/100... Step: 1120... Loss: 4.5445... Val Loss: 4.5476\n",
      "Epoch: 46/100... Step: 1130... Loss: 4.5489... Val Loss: 4.5414\n",
      "Epoch: 46/100... Step: 1140... Loss: 4.5483... Val Loss: 4.5482\n",
      "Epoch: 46/100... Step: 1150... Loss: 4.5555... Val Loss: 4.5470\n",
      "Epoch: 47/100... Step: 1160... Loss: 4.5542... Val Loss: 4.5508\n",
      "Epoch: 47/100... Step: 1170... Loss: 4.5489... Val Loss: 4.5466\n",
      "Epoch: 48/100... Step: 1180... Loss: 4.5499... Val Loss: 4.5482\n",
      "Epoch: 48/100... Step: 1190... Loss: 4.5510... Val Loss: 4.5466\n",
      "Epoch: 48/100... Step: 1200... Loss: 4.5468... Val Loss: 4.5500\n",
      "Epoch: 49/100... Step: 1210... Loss: 4.5469... Val Loss: 4.5520\n",
      "Epoch: 49/100... Step: 1220... Loss: 4.5454... Val Loss: 4.5490\n",
      "Epoch: 50/100... Step: 1230... Loss: 4.5450... Val Loss: 4.5436\n",
      "Epoch: 50/100... Step: 1240... Loss: 4.5465... Val Loss: 4.5462\n",
      "Epoch: 50/100... Step: 1250... Loss: 4.5472... Val Loss: 4.5481\n",
      "Epoch: 51/100... Step: 1260... Loss: 4.5493... Val Loss: 4.5464\n",
      "Epoch: 51/100... Step: 1270... Loss: 4.5475... Val Loss: 4.5503\n",
      "Epoch: 52/100... Step: 1280... Loss: 4.5502... Val Loss: 4.5466\n",
      "Epoch: 52/100... Step: 1290... Loss: 4.5458... Val Loss: 4.5491\n",
      "Epoch: 52/100... Step: 1300... Loss: 4.5446... Val Loss: 4.5480\n",
      "Epoch: 53/100... Step: 1310... Loss: 4.5525... Val Loss: 4.5462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 53/100... Step: 1320... Loss: 4.5473... Val Loss: 4.5482\n",
      "Epoch: 54/100... Step: 1330... Loss: 4.5435... Val Loss: 4.5474\n",
      "Epoch: 54/100... Step: 1340... Loss: 4.5461... Val Loss: 4.5532\n",
      "Epoch: 54/100... Step: 1350... Loss: 4.5485... Val Loss: 4.5491\n",
      "Epoch: 55/100... Step: 1360... Loss: 4.5528... Val Loss: 4.5494\n",
      "Epoch: 55/100... Step: 1370... Loss: 4.5498... Val Loss: 4.5472\n",
      "Epoch: 56/100... Step: 1380... Loss: 4.5437... Val Loss: 4.5485\n",
      "Epoch: 56/100... Step: 1390... Loss: 4.5495... Val Loss: 4.5509\n",
      "Epoch: 56/100... Step: 1400... Loss: 4.5502... Val Loss: 4.5460\n",
      "Epoch: 57/100... Step: 1410... Loss: 4.5469... Val Loss: 4.5486\n",
      "Epoch: 57/100... Step: 1420... Loss: 4.5455... Val Loss: 4.5483\n",
      "Epoch: 58/100... Step: 1430... Loss: 4.5469... Val Loss: 4.5470\n",
      "Epoch: 58/100... Step: 1440... Loss: 4.5492... Val Loss: 4.5501\n",
      "Epoch: 58/100... Step: 1450... Loss: 4.5495... Val Loss: 4.5468\n",
      "Epoch: 59/100... Step: 1460... Loss: 4.5501... Val Loss: 4.5482\n",
      "Epoch: 59/100... Step: 1470... Loss: 4.5449... Val Loss: 4.5459\n",
      "Epoch: 60/100... Step: 1480... Loss: 4.5477... Val Loss: 4.5504\n",
      "Epoch: 60/100... Step: 1490... Loss: 4.5474... Val Loss: 4.5462\n",
      "Epoch: 60/100... Step: 1500... Loss: 4.5477... Val Loss: 4.5456\n",
      "Epoch: 61/100... Step: 1510... Loss: 4.5472... Val Loss: 4.5458\n",
      "Epoch: 61/100... Step: 1520... Loss: 4.5558... Val Loss: 4.5445\n",
      "Epoch: 62/100... Step: 1530... Loss: 4.5500... Val Loss: 4.5529\n",
      "Epoch: 62/100... Step: 1540... Loss: 4.5451... Val Loss: 4.5480\n",
      "Epoch: 62/100... Step: 1550... Loss: 4.5503... Val Loss: 4.5479\n",
      "Epoch: 63/100... Step: 1560... Loss: 4.5439... Val Loss: 4.5479\n",
      "Epoch: 63/100... Step: 1570... Loss: 4.5498... Val Loss: 4.5478\n",
      "Epoch: 64/100... Step: 1580... Loss: 4.5486... Val Loss: 4.5481\n",
      "Epoch: 64/100... Step: 1590... Loss: 4.5510... Val Loss: 4.5455\n",
      "Epoch: 64/100... Step: 1600... Loss: 4.5463... Val Loss: 4.5467\n",
      "Epoch: 65/100... Step: 1610... Loss: 4.5567... Val Loss: 4.5474\n",
      "Epoch: 65/100... Step: 1620... Loss: 4.5507... Val Loss: 4.5429\n",
      "Epoch: 66/100... Step: 1630... Loss: 4.5455... Val Loss: 4.5486\n",
      "Epoch: 66/100... Step: 1640... Loss: 4.5501... Val Loss: 4.5467\n",
      "Epoch: 66/100... Step: 1650... Loss: 4.5518... Val Loss: 4.5457\n",
      "Epoch: 67/100... Step: 1660... Loss: 4.5460... Val Loss: 4.5475\n",
      "Epoch: 67/100... Step: 1670... Loss: 4.5439... Val Loss: 4.5473\n",
      "Epoch: 68/100... Step: 1680... Loss: 4.5486... Val Loss: 4.5477\n",
      "Epoch: 68/100... Step: 1690... Loss: 4.5473... Val Loss: 4.5479\n",
      "Epoch: 68/100... Step: 1700... Loss: 4.5502... Val Loss: 4.5475\n",
      "Epoch: 69/100... Step: 1710... Loss: 4.5469... Val Loss: 4.5448\n",
      "Epoch: 69/100... Step: 1720... Loss: 4.5478... Val Loss: 4.5475\n",
      "Epoch: 70/100... Step: 1730... Loss: 4.5546... Val Loss: 4.5457\n",
      "Epoch: 70/100... Step: 1740... Loss: 4.5483... Val Loss: 4.5470\n",
      "Epoch: 70/100... Step: 1750... Loss: 4.5459... Val Loss: 4.5450\n",
      "Epoch: 71/100... Step: 1760... Loss: 4.5487... Val Loss: 4.5472\n",
      "Epoch: 71/100... Step: 1770... Loss: 4.5551... Val Loss: 4.5502\n",
      "Epoch: 72/100... Step: 1780... Loss: 4.5451... Val Loss: 4.5479\n",
      "Epoch: 72/100... Step: 1790... Loss: 4.5510... Val Loss: 4.5485\n",
      "Epoch: 72/100... Step: 1800... Loss: 4.5459... Val Loss: 4.5470\n",
      "Epoch: 73/100... Step: 1810... Loss: 4.5443... Val Loss: 4.5433\n",
      "Epoch: 73/100... Step: 1820... Loss: 4.5485... Val Loss: 4.5500\n",
      "Epoch: 74/100... Step: 1830... Loss: 4.5538... Val Loss: 4.5486\n",
      "Epoch: 74/100... Step: 1840... Loss: 4.5477... Val Loss: 4.5462\n",
      "Epoch: 74/100... Step: 1850... Loss: 4.5489... Val Loss: 4.5509\n",
      "Epoch: 75/100... Step: 1860... Loss: 4.5537... Val Loss: 4.5494\n",
      "Epoch: 75/100... Step: 1870... Loss: 4.5462... Val Loss: 4.5463\n",
      "Epoch: 76/100... Step: 1880... Loss: 4.5451... Val Loss: 4.5459\n",
      "Epoch: 76/100... Step: 1890... Loss: 4.5489... Val Loss: 4.5491\n",
      "Epoch: 76/100... Step: 1900... Loss: 4.5506... Val Loss: 4.5431\n",
      "Epoch: 77/100... Step: 1910... Loss: 4.5458... Val Loss: 4.5479\n",
      "Epoch: 77/100... Step: 1920... Loss: 4.5478... Val Loss: 4.5458\n",
      "Epoch: 78/100... Step: 1930... Loss: 4.5465... Val Loss: 4.5415\n",
      "Epoch: 78/100... Step: 1940... Loss: 4.5443... Val Loss: 4.5438\n",
      "Epoch: 78/100... Step: 1950... Loss: 4.5516... Val Loss: 4.5485\n",
      "Epoch: 79/100... Step: 1960... Loss: 4.5562... Val Loss: 4.5510\n",
      "Epoch: 79/100... Step: 1970... Loss: 4.5473... Val Loss: 4.5470\n",
      "Epoch: 80/100... Step: 1980... Loss: 4.5525... Val Loss: 4.5442\n",
      "Epoch: 80/100... Step: 1990... Loss: 4.5483... Val Loss: 4.5474\n",
      "Epoch: 80/100... Step: 2000... Loss: 4.5408... Val Loss: 4.5488\n",
      "Epoch: 81/100... Step: 2010... Loss: 4.5475... Val Loss: 4.5461\n",
      "Epoch: 81/100... Step: 2020... Loss: 4.5460... Val Loss: 4.5488\n",
      "Epoch: 82/100... Step: 2030... Loss: 4.5523... Val Loss: 4.5449\n",
      "Epoch: 82/100... Step: 2040... Loss: 4.5475... Val Loss: 4.5480\n",
      "Epoch: 82/100... Step: 2050... Loss: 4.5516... Val Loss: 4.5480\n",
      "Epoch: 83/100... Step: 2060... Loss: 4.5497... Val Loss: 4.5450\n",
      "Epoch: 83/100... Step: 2070... Loss: 4.5467... Val Loss: 4.5465\n",
      "Epoch: 84/100... Step: 2080... Loss: 4.5504... Val Loss: 4.5479\n",
      "Epoch: 84/100... Step: 2090... Loss: 4.5488... Val Loss: 4.5459\n",
      "Epoch: 84/100... Step: 2100... Loss: 4.5533... Val Loss: 4.5478\n",
      "Epoch: 85/100... Step: 2110... Loss: 4.5471... Val Loss: 4.5426\n",
      "Epoch: 85/100... Step: 2120... Loss: 4.5473... Val Loss: 4.5494\n",
      "Epoch: 86/100... Step: 2130... Loss: 4.5492... Val Loss: 4.5481\n",
      "Epoch: 86/100... Step: 2140... Loss: 4.5475... Val Loss: 4.5457\n",
      "Epoch: 86/100... Step: 2150... Loss: 4.5437... Val Loss: 4.5487\n",
      "Epoch: 87/100... Step: 2160... Loss: 4.5497... Val Loss: 4.5446\n",
      "Epoch: 87/100... Step: 2170... Loss: 4.5450... Val Loss: 4.5489\n",
      "Epoch: 88/100... Step: 2180... Loss: 4.5475... Val Loss: 4.5474\n",
      "Epoch: 88/100... Step: 2190... Loss: 4.5498... Val Loss: 4.5472\n",
      "Epoch: 88/100... Step: 2200... Loss: 4.5542... Val Loss: 4.5448\n",
      "Epoch: 89/100... Step: 2210... Loss: 4.5520... Val Loss: 4.5454\n",
      "Epoch: 89/100... Step: 2220... Loss: 4.5494... Val Loss: 4.5490\n",
      "Epoch: 90/100... Step: 2230... Loss: 4.5477... Val Loss: 4.5460\n",
      "Epoch: 90/100... Step: 2240... Loss: 4.5483... Val Loss: 4.5505\n",
      "Epoch: 90/100... Step: 2250... Loss: 4.5486... Val Loss: 4.5472\n",
      "Epoch: 91/100... Step: 2260... Loss: 4.5528... Val Loss: 4.5480\n",
      "Epoch: 91/100... Step: 2270... Loss: 4.5448... Val Loss: 4.5483\n",
      "Epoch: 92/100... Step: 2280... Loss: 4.5527... Val Loss: 4.5476\n",
      "Epoch: 92/100... Step: 2290... Loss: 4.5492... Val Loss: 4.5466\n",
      "Epoch: 92/100... Step: 2300... Loss: 4.5518... Val Loss: 4.5489\n",
      "Epoch: 93/100... Step: 2310... Loss: 4.5425... Val Loss: 4.5503\n",
      "Epoch: 93/100... Step: 2320... Loss: 4.5448... Val Loss: 4.5466\n",
      "Epoch: 94/100... Step: 2330... Loss: 4.5521... Val Loss: 4.5414\n",
      "Epoch: 94/100... Step: 2340... Loss: 4.5525... Val Loss: 4.5420\n",
      "Epoch: 94/100... Step: 2350... Loss: 4.5473... Val Loss: 4.5463\n",
      "Epoch: 95/100... Step: 2360... Loss: 4.5432... Val Loss: 4.5496\n",
      "Epoch: 95/100... Step: 2370... Loss: 4.5525... Val Loss: 4.5494\n",
      "Epoch: 96/100... Step: 2380... Loss: 4.5522... Val Loss: 4.5454\n",
      "Epoch: 96/100... Step: 2390... Loss: 4.5479... Val Loss: 4.5457\n",
      "Epoch: 96/100... Step: 2400... Loss: 4.5492... Val Loss: 4.5453\n",
      "Epoch: 97/100... Step: 2410... Loss: 4.5521... Val Loss: 4.5507\n",
      "Epoch: 97/100... Step: 2420... Loss: 4.5522... Val Loss: 4.5455\n",
      "Epoch: 98/100... Step: 2430... Loss: 4.5479... Val Loss: 4.5497\n",
      "Epoch: 98/100... Step: 2440... Loss: 4.5529... Val Loss: 4.5459\n",
      "Epoch: 98/100... Step: 2450... Loss: 4.5492... Val Loss: 4.5423\n",
      "Epoch: 99/100... Step: 2460... Loss: 4.5483... Val Loss: 4.5471\n",
      "Epoch: 99/100... Step: 2470... Loss: 4.5443... Val Loss: 4.5449\n",
      "Epoch: 100/100... Step: 2480... Loss: 4.5486... Val Loss: 4.5462\n",
      "Epoch: 100/100... Step: 2490... Loss: 4.5490... Val Loss: 4.5435\n",
      "Epoch: 100/100... Step: 2500... Loss: 4.5495... Val Loss: 4.5503\n"
     ]
    }
   ],
   "source": [
    "n_seqs, n_steps = 10, 300\n",
    "#print(encoded.shape)\n",
    "train(net2, encoded,target,epochs=100, n_seqs=n_seqs, n_steps=n_steps, lr=0.0001, cuda=False, print_every=10)\n",
    "#train(net2, real_names, epochs=50, n_seqs=n_seqs, n_steps=n_steps, lr=0.0001, cuda=False, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the name, for saving multiple files\n",
    "model_name2 = 'LSTM_T2.net'\n",
    "\n",
    "checkpoint = {'n_hidden': net1.n_hidden,\n",
    "              'n_layers': net1.n_layers,\n",
    "              'state_dict': net2.state_dict(),\n",
    "              'tokens': net2.chars}\n",
    "\n",
    "with open(model_name2, 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x121030358>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(loss_list,label='Train')\n",
    "plt.plot(loss_val_list,label='Validation')\n",
    "\n",
    "plt.title('Train and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we have loaded in a model that trained over 1 epoch `rnn_1_epoch.net`\n",
    "\n",
    "#https://drive.google.com/file/d/16233aakjLOGu5_r-KwynsP-bC73n2aCy/view?usp=sharing\n",
    "fid2 = '16233aakjLOGu5_r-KwynsP-bC73n2aCy'\n",
    "\n",
    "import wget\n",
    "def download2(fid2, fn):\n",
    "    durl = 'https://drive.google.com/' + 'uc?export=download&id=' + fid2\n",
    "    print('downloading from', durl)\n",
    "    wget.download2(durl, fn)\n",
    "\n",
    "def load_model_T2():\n",
    "    download(fid2, 'LSTM_T2.net')\n",
    "    with open('LSTM_T2.net', 'rb') as f:\n",
    "         checkpoint = torch.load(f)\n",
    "    Loaded_model = Classifier_LSTM(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
    "    Loaded_model.load_state_dict(checkpoint['state_dict'])\n",
    "    \n",
    "    return Loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading from https://drive.google.com/uc?export=download&id=16233aakjLOGu5_r-KwynsP-bC73n2aCy\n",
      "This is a real name\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Change cuda to True if you are using GPU!\n",
    "loaded = load_model_T2()\n",
    "print(net2.is_real_name(\"Allen\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
